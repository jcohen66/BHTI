{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8: 15 - Anatomy of an AI Agent - Enabling ReAct Prompting\n",
    "----------------------------------------------------------------\n",
    "In this lesson, we will enhance our AI Agent to handle ReAct prompting using the latest prompt templating capability. We will ensure the execution loop is properly managed, leveraging the OpenAI Chat Completion API parameter \"stop\" to control the flow. This allows the agent to execute suggested actions/tools, pass the observations back to the prompt, and iterate until the final answer is found.\n",
    "\n",
    "## Objectives\n",
    "* Enable ReAct prompting for the AI Agent.\n",
    "* Use prompt templates to manage the execution loop and handle intermediate actions.\n",
    "* Integrate the OpenAI Chat Completion API \"stop\" parameter to control the prompting process.\n",
    "* Parse responses to identify and process the final answer.\n",
    "\n",
    "## What this session covers:\n",
    "* Defining the current agent structure, including the LLM Client and short-term memory.\n",
    "* Implementing prompt templates to support ReAct prompting.\n",
    "* Managing the execution loop with the \"stop\" parameter in the OpenAI Chat Completion API.\n",
    "* Parsing and validating intermediate and final responses.\n",
    "* Integrating and testing the enhanced Agent with ReAct prompting capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Current Agent Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List\n",
    "import openai\n",
    "\n",
    "class OpenAIChatCompletion:\n",
    "    \"\"\"Interacts with OpenAI's API for chat completions.\"\"\"\n",
    "    def __init__(self, model: str, api_key: str = None, base_url: str = None):\n",
    "        self.client = openai.OpenAI(api_key=api_key, base_url=base_url)\n",
    "        self.model = model\n",
    "\n",
    "    def generate(self, messages: List[str], tools: List[Dict[str, Any]] = None, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Generates a response from OpenAI's API.\"\"\"\n",
    "        params = {'messages': messages, 'model': self.model, 'tools': tools, **kwargs}\n",
    "        response = self.client.chat.completions.create(**params)\n",
    "        return response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short-Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "class ChatMessageMemory:\n",
    "    \"\"\"Manages conversation context.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "    \n",
    "    def add_message(self, message: Dict):\n",
    "        \"\"\"Add a message to memory.\"\"\"\n",
    "        self.messages.append(message)\n",
    "    \n",
    "    def add_messages(self, messages: List[Dict]):\n",
    "        \"\"\"Add multiple messages to memory.\"\"\"\n",
    "        for message in messages:\n",
    "            self.add_message(message)\n",
    "    \n",
    "    def get_messages(self) -> List[Dict]:\n",
    "        \"\"\"Retrieve all messages.\"\"\"\n",
    "        return self.messages.copy()\n",
    "    \n",
    "    def reset_memory(self):\n",
    "        \"\"\"Clear all messages.\"\"\"\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, ValidationError\n",
    "from typing import Callable, Type\n",
    "from inspect import signature\n",
    "\n",
    "class AgentTool:\n",
    "    \"\"\"Encapsulates a Python function with Pydantic validation.\"\"\"\n",
    "    def __init__(self, func: Callable, args_model: Type[BaseModel]):\n",
    "        self.func = func\n",
    "        self.args_model = args_model\n",
    "        self.name = func.__name__\n",
    "        self.description = func.__doc__ or self.args_schema.get('description', '')\n",
    "\n",
    "    def to_openai_function_call_definition(self) -> dict:\n",
    "        \"\"\"Converts the tool to OpenAI Function Calling format.\"\"\"\n",
    "        schema_dict = self.args_schema\n",
    "        description = schema_dict.pop(\"description\", \"\")\n",
    "        return {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": self.name,\n",
    "                \"description\": description,\n",
    "                \"parameters\": schema_dict\n",
    "            }\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def args_schema(self) -> dict:\n",
    "        \"\"\"Returns the tool's function argument schema as a dictionary.\"\"\"\n",
    "        schema = self.args_model.model_json_schema()\n",
    "        schema.pop(\"title\", None)\n",
    "        return schema\n",
    "\n",
    "    def validate_json_args(self, json_string: str) -> bool:\n",
    "        \"\"\"Validate JSON string using the Pydantic model.\"\"\"\n",
    "        try:\n",
    "            validated_args = self.args_model.model_validate_json(json_string)\n",
    "            return isinstance(validated_args, self.args_model)\n",
    "        except ValidationError:\n",
    "            return False\n",
    "\n",
    "    def run(self, *args, **kwargs) -> Any:\n",
    "        \"\"\"Execute the function with validated arguments.\"\"\"\n",
    "        try:\n",
    "            # Handle positional arguments by converting them to keyword arguments\n",
    "            if args:\n",
    "                sig = signature(self.func)\n",
    "                arg_names = list(sig.parameters.keys())\n",
    "                kwargs.update(dict(zip(arg_names, args)))\n",
    "\n",
    "            # Validate arguments with the provided Pydantic schema\n",
    "            validated_args = self.args_model(**kwargs)\n",
    "            return self.func(**validated_args.model_dump())\n",
    "        except ValidationError as e:\n",
    "            raise ValueError(f\"Argument validation failed for tool '{self.name}': {str(e)}\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"An error occurred during the execution of tool '{self.name}': {str(e)}\")\n",
    "\n",
    "    def __call__(self, *args, **kwargs) -> Any:\n",
    "        \"\"\"Allow the AgentTool instance to be called like a regular function.\"\"\"\n",
    "        return self.run(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Tool Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional, Type\n",
    "from pydantic import BaseModel\n",
    "\n",
    "def check_docstring(func: Callable):\n",
    "    \"\"\"Ensure the function has a docstring.\"\"\"\n",
    "    if not func.__doc__:\n",
    "        raise ValueError(f\"Function '{func.__name__}' must have a docstring.\")\n",
    "\n",
    "def Tool(func: Optional[Callable] = None, *, args_model: Type[BaseModel]) -> AgentTool:\n",
    "    \"\"\"Decorator to wrap a function with an AgentTool instance.\"\"\"\n",
    "    def decorator(f: Callable) -> AgentTool:\n",
    "        check_docstring(f)\n",
    "        return AgentTool(f, args_model=args_model)\n",
    "    return decorator(func) if func else decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Tool Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional\n",
    "import json\n",
    "\n",
    "class AgentToolExecutor:\n",
    "    \"\"\"Manages tool registration and execution.\"\"\"\n",
    "    \n",
    "    def __init__(self, tools: Optional[List[AgentTool]] = None):\n",
    "        self.tools: Dict[str, AgentTool] = {}\n",
    "        if tools:\n",
    "            for tool in tools:\n",
    "                self.register_tool(tool)\n",
    "    \n",
    "    def register_tool(self, tool: AgentTool):\n",
    "        \"\"\"Registers a tool.\"\"\"\n",
    "        if tool.name in self.tools:\n",
    "            raise ValueError(f\"Tool '{tool.name}' is already registered.\")\n",
    "        self.tools[tool.name] = tool\n",
    "      \n",
    "    def execute(self, tool_name: str, tool_args: str) -> Any:\n",
    "        \"\"\"Executes a tool by name with given arguments.\"\"\"\n",
    "        print(f\"Checking if {tool_name} tool exists..\")\n",
    "        tool = self.tools.get(tool_name)\n",
    "        if not tool:\n",
    "            raise ValueError(f\"Tool '{tool_name}' not found.\")\n",
    "        try:\n",
    "            print(f\"Validating {tool_name} suggested args {tool_args}\")\n",
    "            if tool.validate_json_args(tool_args):\n",
    "                tool_args_dict = json.loads(tool_args)\n",
    "                print(f\"Executing {tool_name} with args: {tool_args}\")\n",
    "                return tool.run(**tool_args_dict)\n",
    "            else:\n",
    "                raise ValueError(f\"Error validating tool '{tool_name}' arguments.\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error executing tool '{tool_name}': {e}\") from e\n",
    "    \n",
    "    def get_tool_names(self) -> List[str]:\n",
    "        \"\"\"Returns a list of all registered tool names.\"\"\"\n",
    "        return list(self.tools.keys())\n",
    "    \n",
    "    def get_tool_details(self) -> str:\n",
    "        \"\"\"Returns details of all registered tools.\"\"\"\n",
    "        tools_info = [f\"{tool.name}: {tool.description} Args schema: {tool.args_schema['properties']}\" for tool in self.tools.values()]\n",
    "        return '\\n'.join(tools_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class StringPromptTemplate:\n",
    "    \"\"\"Handles dynamic prompt formatting for an AI agent.\"\"\"\n",
    "\n",
    "    def __init__(self, template: str):\n",
    "        \"\"\"Initializes the prompt template and extracts variables.\"\"\"\n",
    "        self.template = template\n",
    "        self.variables = {}\n",
    "        self.required_variables = self.extract_variables()\n",
    "\n",
    "    def extract_variables(self):\n",
    "        \"\"\"Extracts placeholders from the template.\"\"\"\n",
    "        return set(re.findall(r'\\{(.*?)\\}', self.template))\n",
    "\n",
    "    def update_variables(self, **kwargs):\n",
    "        \"\"\"Updates template variables.\"\"\"\n",
    "        self.variables.update(kwargs)\n",
    "        self.required_variables -= set(kwargs.keys())\n",
    "\n",
    "    def format_prompt(self, **kwargs):\n",
    "        \"\"\"Generates a formatted prompt and tracks remaining variables.\"\"\"\n",
    "        combined_variables = {**self.variables, **kwargs}\n",
    "        self.required_variables -= set(kwargs.keys())\n",
    "        return self.template.format(**combined_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Integrates key components and manages tool executions.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client, system_message: Dict[str, str], max_iterations: int = 10, tools: Optional[List[AgentTool]] = None, prompt_template: StringPromptTemplate = None):\n",
    "        self.llm_client = llm_client\n",
    "        self.executor = AgentToolExecutor()\n",
    "        self.memory = ChatMessageMemory()\n",
    "        self.system_message = system_message\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tool_history = []\n",
    "        self.function_calls = None\n",
    "        self.prompt_template = prompt_template\n",
    "\n",
    "        if tools:\n",
    "            for tool in tools:\n",
    "                self.executor.register_tool(tool)\n",
    "            self.function_calls = [tool.to_openai_function_call_definition() for tool in tools]\n",
    "\n",
    "        tool_details = self.executor.get_tool_details()\n",
    "        tool_names = ' or '.join(self.executor.get_tool_names())\n",
    "        self.prompt_template.update_variables(\n",
    "            system_message=self.system_message,\n",
    "            tool_details=tool_details,\n",
    "            tool_names=tool_names\n",
    "        )\n",
    "\n",
    "    def run(self, task: str):\n",
    "        \"\"\"Generates responses, manages tool calls, and updates memory.\"\"\"\n",
    "        self.memory.add_message({\"role\": \"user\", \"content\": task})\n",
    "\n",
    "        for _ in range(self.max_iterations):\n",
    "            chat_history = self.messages_to_string()\n",
    "            formatted_message = self.prompt_template.format_prompt(chat_history=chat_history, user_input=task)\n",
    "            messages = [{\"role\": \"user\", \"content\": formatted_message}]\n",
    "            response = self.llm_client.generate(messages=messages)\n",
    "            action_dict = self.parse_response(response)\n",
    "\n",
    "            if action_dict:\n",
    "                action_name = action_dict[\"name\"]\n",
    "                action_arguments = action_dict[\"args_json\"]\n",
    "                execution_results = self.executor.execute(action_name, action_arguments)\n",
    "                return execution_results\n",
    "            else:\n",
    "                logger.info(\"Agent is responding directly.\")\n",
    "                self.memory.add_messages(user_message={\"role\": \"user\", \"content\": task}, assistant_message=response)\n",
    "                return response\n",
    "\n",
    "    def parse_response(self, response: Dict):\n",
    "        \"\"\"Extracts tools or continues the conversation.\"\"\"\n",
    "        import regex\n",
    "\n",
    "        pattern = regex.compile(r'\\{(?:[^{}]|(?R))*\\}')  # Supports nested structures\n",
    "        message_content = response.content\n",
    "        # Unescape backslashes\n",
    "        message_content = message_content.replace('\\\\\\\\n', '\\\\n').replace('\\\\n', '\\n').replace('\\\\\\'', '\\'').replace('\\\\\\\\', '\\\\')\n",
    "        # Replace double curly braces with single curly braces\n",
    "        message_content = message_content.replace('{{', '{').replace('}}', '}')\n",
    "\n",
    "        match = pattern.search(message_content)\n",
    "        if match:\n",
    "            action_content = match.group()\n",
    "            try:\n",
    "                action_dict = json.loads(action_content.strip())\n",
    "                action_dict['args_json'] = json.dumps(action_dict[\"arguments\"])\n",
    "                return action_dict\n",
    "            except json.JSONDecodeError:\n",
    "                raise ValueError(\"Invalid JSON in action content\")\n",
    "        return None\n",
    "    \n",
    "    def messages_to_string(self) -> str:\n",
    "        \"\"\"Converts messages to a string.\"\"\"\n",
    "        formatted_messages = []\n",
    "        for message in self.memory.get_messages():\n",
    "            formatted_messages.append(f\"{message['role'].capitalize()}: {message['content']}\")\n",
    "        return \"\\n\".join(formatted_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Agent Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Dict, List, Optional\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Dict\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Basic Agent class responsible for integrating key components such as the LLM client, tools, memory, and managing tool executions.\n",
    "    \"\"\"\n",
    "    def __init__(self, llm_client, system_message: Dict[str, str], max_iterations: int = 10, tools: Optional[List[AgentTool]] = None, prompt_template : StringPromptTemplate=None):\n",
    "        self.llm_client = llm_client\n",
    "        self.executor = AgentToolExecutor()\n",
    "        self.memory = ChatMessageMemory()\n",
    "        self.system_message = system_message\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tool_history = []\n",
    "        self.function_calls = None\n",
    "        self.prompt_template = prompt_template  # Instance of StringPromptTemplate or similar\n",
    "        \n",
    "        # Register each tool passed to the Agent using the executor\n",
    "        if tools:\n",
    "            for tool in tools:\n",
    "                # Register Agent Tools\n",
    "                self.executor.register_tool(tool)\n",
    "                # Convert Agent Tools\n",
    "                self.function_calls = [tool.to_openai_function_call_definition() for tool in tools]\n",
    "        \n",
    "        # Pre-fill the prompt template with initial variables\n",
    "        tool_details=self.executor.get_tool_details(),\n",
    "        tool_names=' or '.join(self.executor.get_tool_names())\n",
    "        self.prompt_template.update_variables(\n",
    "            system_message=self.system_message,\n",
    "            tool_details=tool_details,\n",
    "            tool_names=tool_names\n",
    "        )\n",
    "\n",
    "    def run(self, task:str):\n",
    "        # Get Chat History\n",
    "        chat_history = self.messages_to_string()\n",
    "        # Initialize ReAct Loop\n",
    "        react_loop = \"\"\n",
    "\n",
    "        # Showing Initial Task\n",
    "        print(f\"Question: {task}\")\n",
    "\n",
    "        for _ in range(self.max_iterations):\n",
    "            # Generate a dynamic prompt using variables\n",
    "            formatted_message = self.prompt_template.format_prompt(chat_history=chat_history,user_input=task,react_loop=react_loop)\n",
    "            # Define everything as a user message\n",
    "            messages = [{\"role\":\"user\", \"content\": formatted_message}]\n",
    "            \n",
    "            # Instruct LLM to choose the right tool and respond with a structured output\n",
    "            response = self.llm_client.generate(messages=messages,stop=[\"\\nObservation:\"],)\n",
    "\n",
    "            # Parse response and extract tools if any\n",
    "            action_dict = self.parse_response(response)\n",
    "\n",
    "            if action_dict:\n",
    "                current_thought = f\"{response.content}\"\n",
    "                print(current_thought)\n",
    "                \n",
    "                action_name = action_dict[\"name\"]\n",
    "                action_arguments = action_dict[\"args_json\"]\n",
    "                execution_results = self.executor.execute(action_name, action_arguments)\n",
    "                \n",
    "                current_observation = f\"Observation: {execution_results}\"\n",
    "                print(current_observation)\n",
    "                react_loop += (current_thought + current_observation)\n",
    "            else:\n",
    "                message_content = response.content\n",
    "                print(message_content)\n",
    "                if 'final answer' in str(message_content).lower():\n",
    "                    final_message = str(message_content).lower().split(\"final answer:\")[-1].strip()\n",
    "                    response = {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": final_message\n",
    "                    }\n",
    "                \n",
    "                logger.info(\"Agent is responding directly.\")\n",
    "                self.memory.add_messages([{\"role\":\"user\",\"content\": task},response])\n",
    "                return response    \n",
    "\n",
    "    def parse_response(self, response: Dict):\n",
    "        \"\"\"Extracts tools or continues the conversation.\"\"\"\n",
    "        import regex\n",
    "\n",
    "        pattern = regex.compile(r'\\{(?:[^{}]|(?R))*\\}')  # Supports nested structures\n",
    "        message_content = response.content\n",
    "        # Unescape backslashes\n",
    "        message_content = message_content.replace('\\\\\\\\n', '\\\\n').replace('\\\\n', '\\n').replace('\\\\\\'', '\\'').replace('\\\\\\\\', '\\\\')\n",
    "        # Replace double curly braces with single curly braces\n",
    "        message_content = message_content.replace('{{', '{').replace('}}', '}')\n",
    "\n",
    "        match = pattern.search(message_content)\n",
    "        if match:\n",
    "            action_content = match.group()\n",
    "            try:\n",
    "                action_dict = json.loads(action_content.strip())\n",
    "                action_dict['args_json'] = json.dumps(action_dict[\"arguments\"])\n",
    "                return action_dict\n",
    "            except json.JSONDecodeError:\n",
    "                raise ValueError(\"Invalid JSON in action content\")\n",
    "        return None\n",
    "    \n",
    "    def messages_to_string(self) -> str:\n",
    "        \"\"\"\n",
    "        Converts a list of message objects or dictionaries into a multi-line string representation.\n",
    "        \"\"\"\n",
    "        formatted_messages = []\n",
    "\n",
    "        for message in self.memory.get_messages():\n",
    "            if isinstance(message, BaseModel):\n",
    "                message = message.model_dump()\n",
    "            formatted_messages.append(f\"{message[\"role\"].capitalize()}: {message[\"content\"]}\")\n",
    "        return \"\\n\".join(formatted_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update LLM Client\n",
    "\n",
    "I updated the LLM Client class to use the stop parameter in [OpenAI's Chat Completion API](https://platform.openai.com/docs/api-reference/chat/create?ref=blog.openthreatresearch.com), stopping token generation at \\nObservation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List\n",
    "import openai\n",
    "\n",
    "class OpenAIChatCompletion:\n",
    "    \"\"\"Handles interaction with OpenAI's API for generating chat completions.\"\"\"\n",
    "    def __init__(self, model: str, api_key: str = None, base_url: str = None):\n",
    "        self.client = openai.OpenAI(api_key=api_key, base_url=base_url)\n",
    "        self.model = model\n",
    "\n",
    "    def generate(self, messages: List[str], tools: List[Dict[str, Any]] = None, stop: List[str] = None, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Generate a response from OpenAI's API based on input messages.\"\"\"\n",
    "        params = {\n",
    "            'messages': messages,\n",
    "            'model': self.model,\n",
    "            'tools': tools,\n",
    "            'stop': stop,\n",
    "            **kwargs\n",
    "        }\n",
    "        response = self.client.chat.completions.create(**params)\n",
    "        return response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test ReAct Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "import random\n",
    "\n",
    "class GetWeatherSchema(BaseModel):\n",
    "    \"\"\"Get weather information based on location.\"\"\"\n",
    "    location: str = Field(description=\"Location to get weather for\")\n",
    "\n",
    "@Tool(args_model=GetWeatherSchema)\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Gets weather information.\"\"\"\n",
    "    temperature = random.randint(60, 80)\n",
    "    return f\"{location}: {temperature}F.\"\n",
    "\n",
    "class JumpSchema(BaseModel):\n",
    "    \"\"\"Jump a specific distance\"\"\"\n",
    "    distance: str = Field(description=\"Specific distance to jump for\")\n",
    "\n",
    "@Tool(args_model=JumpSchema)\n",
    "def jump(distance: str) -> str:\n",
    "    \"\"\"Jumps a specific distance.\"\"\"\n",
    "    return f\"I jumped the following distance {distance}\"\n",
    "\n",
    "tools = [get_weather, jump]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import ReAct Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ReAct_Template import STRING_PROMPT_TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = StringPromptTemplate(STRING_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API from environment variable\n",
    "# import os\n",
    "# api_key = os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "api_key=\"\"\n",
    "\n",
    "client = OpenAIChatCompletion(\n",
    "    base_url='https://api.openai.com/v1',\n",
    "    model='gpt-4o',\n",
    "    api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define System messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system message\n",
    "system_message = {\"role\": \"system\", \"content\": \"You are a weather assistant.\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Agent with the LLM client and system message\n",
    "agent = Agent(llm_client=client, system_message=system_message, tools=tools, prompt_template=prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send a User Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the weather in New York?\n",
      "Thought: The user wants to know the weather in New York. I'll use the get_weather tool to get the required information.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "    \"name\": \"get_weather\",\n",
      "    \"arguments\": {\n",
      "        \"location\": \"New York\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "Checking if get_weather tool exists..\n",
      "Validating get_weather suggested args {\"location\": \"New York\"}\n",
      "Executing get_weather with args: {\"location\": \"New York\"}\n",
      "Observation: New York: 60F.\n",
      "Thought: I know what to respond as final answer. Using tool to provide final answer.\n",
      "Final Answer: The current weather in New York is 60°F.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant', 'content': 'the current weather in new york is 60°f.'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"What is the weather in New York?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
