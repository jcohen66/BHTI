{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8: 7 - Anatomy of an AI Agent - Tool Execution via OpenAI Function Calling\n",
    "---------------------------------------------------------------------------------\n",
    "In this lesson, we will enhance our AI Agent by enabling tool execution using [OpenAI's Function Calling](https://platform.openai.com/docs/guides/function-calling) capability. This feature allows the model to understand function signatures, detect when a tool needs to be used based on user intent, choose the right tool, and return the function signature to be executed by the AI Agent.\n",
    "\n",
    "## Objectives\n",
    "* Understand the concept of tool execution using OpenAI Function Calling.\n",
    "* Update the LLM Client to accept and utilize tools.\n",
    "* Integrate tool execution capabilities into the existing AI Agent structure.\n",
    "* Test the AI Agent's ability to choose and execute tools based on user input.\n",
    "\n",
    "## What this session covers:\n",
    "* Installing necessary libraries for OpenAI integration.\n",
    "* Defining the current agent structure, including the LLM Client and short-term memory.\n",
    "* Implementing OpenAI's Function Calling for tool execution.\n",
    "* Updating the agent to hold and use tools.\n",
    "* Initializing and testing the enhanced AI Agent with example tools and interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/miniconda3/envs/autogen/lib/python3.11/site-packages (1.53.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /opt/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (4.66.6)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/miniconda3/envs/autogen/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n"
     ]
    }
   ],
   "source": [
    "! pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Current Agent Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List\n",
    "import openai\n",
    "\n",
    "class OpenAIChatCompletion:\n",
    "    \"\"\"\n",
    "    Interacts with OpenAI's API for chat completions.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: str, api_key: str = None, base_url: str = None):\n",
    "        \"\"\"\n",
    "        Initialize with model, API key, and base URL.\n",
    "        \"\"\"\n",
    "        self.client = openai.OpenAI(api_key=api_key, base_url=base_url)\n",
    "        self.model = model\n",
    "\n",
    "    def generate(self, messages: List[Dict], **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate a response from input messages.\n",
    "        \"\"\"\n",
    "        params = {'messages': messages, 'model': self.model, **kwargs}\n",
    "        response = self.client.chat.completions.create(**params)\n",
    "        return response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short-Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "class ChatMessageMemory:\n",
    "    \"\"\"Manages conversation context.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "    \n",
    "    def add_message(self, message: Dict):\n",
    "        \"\"\"Add a message to memory.\"\"\"\n",
    "        self.messages.append(message)\n",
    "    \n",
    "    def add_messages(self, messages: List[Dict]):\n",
    "        \"\"\"Add multiple messages to memory.\"\"\"\n",
    "        for message in messages:\n",
    "            self.add_message(message)\n",
    "    \n",
    "    def get_messages(self) -> List[Dict]:\n",
    "        \"\"\"Retrieve all messages.\"\"\"\n",
    "        return self.messages.copy()\n",
    "    \n",
    "    def reset_memory(self):\n",
    "        \"\"\"Clear all messages.\"\"\"\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Integrates LLM client, tools, and memory.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client, system_message: Dict[str, str], tools=None):\n",
    "        self.llm_client = llm_client\n",
    "        self.tools = tools\n",
    "        self.memory = ChatMessageMemory()\n",
    "        self.system_message = system_message\n",
    "\n",
    "    def run(self, user_message: Dict[str, str]):\n",
    "        \"\"\"Generate a response using LLM client and store context.\"\"\"\n",
    "        self.memory.add_message(user_message)\n",
    "        chat_history = [self.system_message] + self.memory.get_messages()\n",
    "        response = self.llm_client.generate(chat_history)\n",
    "        self.memory.add_message(response)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Function Calling\n",
    "\n",
    "One reliable method for choosing tools and returning structured outputs is OpenAI's Function Calling. Introduced on June 13, 2023, this feature allows developers to describe functions to models trained to generate a JSON object with the necessary arguments based on user input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update OpenAI Client to Accept Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List\n",
    "import openai\n",
    "\n",
    "class OpenAIChatCompletion:\n",
    "    \"\"\"Interacts with OpenAI's API for chat completions.\"\"\"\n",
    "    def __init__(self, model: str, api_key: str = None, base_url: str = None):\n",
    "        self.client = openai.OpenAI(api_key=api_key, base_url=base_url)\n",
    "        self.model = model\n",
    "\n",
    "    def generate(self, messages: List[str], tools: List[Dict[str, Any]] = None, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Generates a response from OpenAI's API.\"\"\"\n",
    "        params = {'messages': messages, 'model': self.model, 'tools': tools, **kwargs}\n",
    "        response = self.client.chat.completions.create(**params)\n",
    "        return response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Agent to Hold Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Integrates LLM client, tools, and memory.\"\"\"\n",
    "    def __init__(self, llm_client, system_message: Dict[str, str], tools=None):\n",
    "        self.llm_client = llm_client\n",
    "        self.tools = tools\n",
    "        self.memory = ChatMessageMemory()\n",
    "        self.system_message = system_message\n",
    "\n",
    "    def run(self, user_message: Dict[str, str]):\n",
    "        self.memory.add_message(user_message)\n",
    "        chat_history = [self.system_message] + self.memory.get_messages()\n",
    "        response = self.llm_client.generate(chat_history, tools=self.tools)\n",
    "        self.memory.add_message(response)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Current Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Dummy Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Gets weather information.\"\"\"\n",
    "    temperature = random.randint(60, 80)\n",
    "    return f\"{location}: {temperature}F.\"\n",
    "\n",
    "def jump(distance: str) -> str:\n",
    "    \"\"\"Jumps a specific distance.\"\"\"\n",
    "    return f\"I jumped the following distance {distance}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Python Function Signature in OAI Function Calling Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_weather_func_dict = {\n",
    "    'type': 'function',\n",
    "    'function': {\n",
    "        'name': 'get_weather',\n",
    "        'description': 'Get weather information based on location.',\n",
    "        'parameters': {\n",
    "            'properties': {'location': {'type': 'string'}},\n",
    "            'required': ['location'],\n",
    "            'type': 'object'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "jump_func_dict = {\n",
    "    'type': 'function',\n",
    "    'function': {\n",
    "        'name': 'jump',\n",
    "        'description': 'Jump a specific distance.',\n",
    "        'parameters': {\n",
    "            'properties': {'distance': {'type': 'string'}},\n",
    "            'required': ['distance'],\n",
    "            'type': 'object'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "tools = [get_weather_func_dict, jump_func_dict]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API from environment variable\n",
    "import os\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAIChatCompletion(\n",
    "    base_url='https://api.openai.com/v1',\n",
    "    model='gpt-4',\n",
    "    api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define System messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system message\n",
    "system_message = {\"role\": \"system\", \"content\": \"You are a weather assistant.\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Agent with the LLM client, system message, and tools\n",
    "agent = Agent(llm_client=client, system_message=system_message, tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send a User Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_zTIzeWVg6bp5Fa9zPJAXWda1', function=Function(arguments='{\\n  \"location\": \"Virginia\"\\n}', name='get_weather'), type='function')])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a user message\n",
    "user_message = {\"role\": \"user\", \"content\": \"What is the weather in Virginia?\"}\n",
    "\n",
    "# Generate a response using the agent\n",
    "response = agent.run(user_message)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': None,\n",
       " 'refusal': None,\n",
       " 'role': 'assistant',\n",
       " 'tool_calls': [{'id': 'call_zTIzeWVg6bp5Fa9zPJAXWda1',\n",
       "   'function': {'arguments': '{\\n  \"location\": \"Virginia\"\\n}',\n",
       "    'name': 'get_weather'},\n",
       "   'type': 'function'}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Tool Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'location': 'Virginia'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "tool_response = response.tool_calls[0]\n",
    "tool_arguments = json.loads(tool_response.function.arguments)\n",
    "tool_arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Virginia: 67F.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_execution_results = get_weather(**tool_arguments)\n",
    "tool_execution_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
