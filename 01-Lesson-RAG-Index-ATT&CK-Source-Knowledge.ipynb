{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8: 1 - Indexing Data into a Vector Database and RAG with Langchain\n",
    "----------------------------------------------------------------------------\n",
    "In this lesson, we will demonstrate the process of indexing data into a vector database and using Langchain's Q&A chain to seamlessly implement the Retrieval Augmented Generation (RAG) flow for Knowledge Enhanced LLMs. We'll begin by downloading ATT&CK Enterprise data in STIX format using the attackcti Python library. Next, we'll extract all groups and techniques used by these groups to create markdown files simulating an Intel repository where threat intelligence analysts record notes about tracked threat actors. After loading and tokenizing these markdown files, we'll set them up in a FAISS database to generate embeddings. We will then apply a similarity search method to find relevant documents. Finally, we'll set the vector database as a retriever, initialize a retriever chain, and use it as a RAG chain with Langchain to automate retrieval and context addition, allowing the LLM to provide informed answers.\n",
    "\n",
    "## Objectives\n",
    "* Understand the process of indexing data into a vector database.\n",
    "* Learn how to set the vector database as a retriever and initialize a retriever chain for RAG.\n",
    "* Simulate an Intel repository using markdown files.\n",
    "* Generate embeddings and perform similarity searches to find relevant data.\n",
    "* Automate context addition to user prompts for improved LLM responses.\n",
    "\n",
    "## What this session covers:\n",
    "* Downloading and organizing ATT&CK Enterprise data using the attackcti Python library.\n",
    "* Extracting and converting group and technique data into markdown files.\n",
    "* Simulating an Intel repository with markdown files.\n",
    "* Loading, tokenizing, and embedding data in a FAISS database.\n",
    "* Performing similarity searches to find relevant documents.\n",
    "* Setting the vector database as a retriever.\n",
    "* Initializing and utilizing a retriever chain for RAG with Langchain.\n",
    "* Interacting with the LLM for enhanced, context-rich answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting attackcti\n",
      "  Using cached attackcti-0.4.4-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting stix2 (from attackcti)\n",
      "  Using cached stix2-3.0.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting taxii2-client (from attackcti)\n",
      "  Using cached taxii2_client-2.3.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pydantic in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from attackcti) (2.9.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic->attackcti) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic->attackcti) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\jcohe\\appdata\\roaming\\python\\python311\\site-packages (from pydantic->attackcti) (4.12.2)\n",
      "Requirement already satisfied: pytz in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stix2->attackcti) (2024.2)\n",
      "Requirement already satisfied: requests in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stix2->attackcti) (2.32.3)\n",
      "Collecting simplejson (from stix2->attackcti)\n",
      "  Using cached simplejson-3.19.3-cp311-cp311-win_amd64.whl.metadata (3.2 kB)\n",
      "Collecting stix2-patterns>=1.2.0 (from stix2->attackcti)\n",
      "  Using cached stix2_patterns-2.0.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: six in c:\\users\\jcohe\\appdata\\roaming\\python\\python311\\site-packages (from taxii2-client->attackcti) (1.16.0)\n",
      "Collecting antlr4-python3-runtime~=4.9.0 (from stix2-patterns>=1.2.0->stix2->attackcti)\n",
      "  Using cached antlr4_python3_runtime-4.9.3-py3-none-any.whl\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->stix2->attackcti) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->stix2->attackcti) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->stix2->attackcti) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->stix2->attackcti) (2024.8.30)\n",
      "Using cached attackcti-0.4.4-py3-none-any.whl (22 kB)\n",
      "Using cached stix2-3.0.1-py2.py3-none-any.whl (177 kB)\n",
      "Using cached taxii2_client-2.3.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached stix2_patterns-2.0.0-py2.py3-none-any.whl (65 kB)\n",
      "Using cached simplejson-3.19.3-cp311-cp311-win_amd64.whl (75 kB)\n",
      "Installing collected packages: antlr4-python3-runtime, stix2-patterns, simplejson, taxii2-client, stix2, attackcti\n",
      "Successfully installed antlr4-python3-runtime-4.9.3 attackcti-0.4.4 simplejson-3.19.3 stix2-3.0.1 stix2-patterns-2.0.0 taxii2-client-2.3.0\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "# !pip install openai\n",
    "# !pip install langchain\n",
    "# !pip install langchain_openai\n",
    "# !pip install -qU langchain_community\n",
    "# !pip install faiss-cpu\n",
    "# !pip install pydantic\n",
    "!pip install attackcti\n",
    "# !pip install unstructured\n",
    "# !pip install markdown\n",
    "# !pip install tiktoken\n",
    "# !pip install langchain_huggingface\n",
    "# !pip install jinja2\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Initial Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define a few variables\n",
    "current_directory = os.path.dirname(\"__file__\")\n",
    "data_directory = os.path.join(current_directory, \"data\")\n",
    "documents_directory = os.path.join(data_directory, \"documents\")\n",
    "templates_directory = os.path.join(current_directory, \"templates\")\n",
    "group_template = os.path.join(templates_directory, \"group.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download ATT&CK STIX Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded enterprise-attack.json to data\\attack\\v15.1\n",
      "Downloaded mobile-attack.json to data\\attack\\v15.1\n",
      "Downloaded ics-attack.json to data\\attack\\v15.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'enterprise': 'data\\\\attack\\\\v15.1\\\\enterprise-attack.json',\n",
       " 'mobile': 'data\\\\attack\\\\v15.1\\\\mobile-attack.json',\n",
       " 'ics': 'data\\\\attack\\\\v15.1\\\\ics-attack.json'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from attackcti.utils.downloader import STIXDownloader\n",
    "\n",
    "stix20_downloader = STIXDownloader(download_dir=\"./data/attack\", stix_version=\"2.0\")\n",
    "\n",
    "stix20_downloader.download_all_domains(release=\"15.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'enterprise': 'data\\\\attack\\\\v15.1\\\\enterprise-attack.json',\n",
       " 'mobile': 'data\\\\attack\\\\v15.1\\\\mobile-attack.json',\n",
       " 'ics': 'data\\\\attack\\\\v15.1\\\\ics-attack.json'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stix20_downloader.downloaded_file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize ATT&CK Python Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attackcti import attack_client\n",
    "\n",
    "lift = attack_client(local_paths=stix20_downloader.downloaded_file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Techniques Used by ATT&CK Groups\n",
    "Gettings technique STIX objects used by all groups accross all ATT&CK matrices.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'intrusion-set',\n",
       " 'id': 'intrusion-set--01e28736-2ffc-455b-9880-ed4d1407ae07',\n",
       " 'created_by_ref': 'identity--c78cb6e5-0c4b-4611-8297-d1b8b55e40b5',\n",
       " 'created': '2021-01-06T17:46:35.134Z',\n",
       " 'modified': '2024-04-17T22:10:56.266Z',\n",
       " 'name': 'Indrik Spider',\n",
       " 'description': '[Indrik Spider](https://attack.mitre.org/groups/G0119) is a Russia-based cybercriminal group that has been active since at least 2014. [Indrik Spider](https://attack.mitre.org/groups/G0119) initially started with the [Dridex](https://attack.mitre.org/software/S0384) banking Trojan, and then by 2017 they began running ransomware operations using [BitPaymer](https://attack.mitre.org/software/S0570), [WastedLocker](https://attack.mitre.org/software/S0612), and Hades ransomware. Following U.S. sanctions and an indictment in 2019, [Indrik Spider](https://attack.mitre.org/groups/G0119) changed their tactics and diversified their toolset.(Citation: Crowdstrike Indrik November 2018)(Citation: Crowdstrike EvilCorp March 2021)(Citation: Treasury EvilCorp Dec 2019)',\n",
       " 'aliases': ['Indrik Spider', 'Evil Corp', 'Manatee Tempest', 'DEV-0243'],\n",
       " 'external_references': [{'source_name': 'mitre-attack',\n",
       "   'url': 'https://attack.mitre.org/groups/G0119',\n",
       "   'external_id': 'G0119'},\n",
       "  {'source_name': 'Evil Corp',\n",
       "   'description': '(Citation: Crowdstrike EvilCorp March 2021)(Citation: Treasury EvilCorp Dec 2019)'},\n",
       "  {'source_name': 'Manatee Tempest',\n",
       "   'description': '(Citation: Microsoft Threat Actor Naming July 2023)'},\n",
       "  {'source_name': 'DEV-0243',\n",
       "   'description': '(Citation: Microsoft Threat Actor Naming July 2023)'},\n",
       "  {'source_name': 'Crowdstrike Indrik November 2018',\n",
       "   'description': 'Frankoff, S., Hartley, B. (2018, November 14). Big Game Hunting: The Evolution of INDRIK SPIDER From Dridex Wire Fraud to BitPaymer Targeted Ransomware. Retrieved January 6, 2021.',\n",
       "   'url': 'https://www.crowdstrike.com/blog/big-game-hunting-the-evolution-of-indrik-spider-from-dridex-wire-fraud-to-bitpaymer-targeted-ransomware/'},\n",
       "  {'source_name': 'Microsoft Threat Actor Naming July 2023',\n",
       "   'description': 'Microsoft . (2023, July 12). How Microsoft names threat actors. Retrieved November 17, 2023.',\n",
       "   'url': 'https://learn.microsoft.com/en-us/microsoft-365/security/intelligence/microsoft-threat-actor-naming?view=o365-worldwide'},\n",
       "  {'source_name': 'Crowdstrike EvilCorp March 2021',\n",
       "   'description': 'Podlosky, A., Feeley, B. (2021, March 17). INDRIK SPIDER Supersedes WastedLocker with Hades Ransomware to Circumvent OFAC Sanctions. Retrieved September 15, 2021.',\n",
       "   'url': 'https://www.crowdstrike.com/blog/hades-ransomware-successor-to-indrik-spiders-wastedlocker/'},\n",
       "  {'source_name': 'Treasury EvilCorp Dec 2019',\n",
       "   'description': 'U.S. Department of Treasury. (2019, December 5). Treasury Sanctions Evil Corp, the Russia-Based Cybercriminal Group Behind Dridex Malware. Retrieved September 15, 2021.',\n",
       "   'url': 'https://home.treasury.gov/news/press-releases/sm845'}],\n",
       " 'object_marking_refs': ['marking-definition--fa42a846-8d90-4e51-bc29-71d5b4802168'],\n",
       " 'x_mitre_attack_spec_version': '3.2.0',\n",
       " 'x_mitre_contributors': ['Jennifer Kim Roman, CrowdStrike'],\n",
       " 'x_mitre_deprecated': False,\n",
       " 'x_mitre_domains': ['enterprise-attack'],\n",
       " 'x_mitre_modified_by_ref': 'identity--c78cb6e5-0c4b-4611-8297-d1b8b55e40b5',\n",
       " 'x_mitre_version': '4.0',\n",
       " 'technique_ref': 'attack-pattern--65f2d882-3f41-4d48-8a06-29af77ec9f90',\n",
       " 'relationship_description': '[Indrik Spider](https://attack.mitre.org/groups/G0119) used [Cobalt Strike](https://attack.mitre.org/software/S0154) to carry out credential dumping using ProcDump.(Citation: Symantec WastedLocker June 2020)',\n",
       " 'relationship_id': 'relationship--000aa4d0-315e-40d7-b2b6-76e91ecf0fe8',\n",
       " 'revoked': False,\n",
       " 'technique': 'LSASS Memory',\n",
       " 'technique_description': \"Adversaries may attempt to access credential material stored in the process memory of the Local Security Authority Subsystem Service (LSASS). After a user logs on, the system generates and stores a variety of credential materials in LSASS process memory. These credential materials can be harvested by an administrative user or SYSTEM and used to conduct [Lateral Movement](https://attack.mitre.org/tactics/TA0008) using [Use Alternate Authentication Material](https://attack.mitre.org/techniques/T1550).\\n\\nAs well as in-memory techniques, the LSASS process memory can be dumped from the target host and analyzed on a local system.\\n\\nFor example, on the target host use procdump:\\n\\n* <code>procdump -ma lsass.exe lsass_dump</code>\\n\\nLocally, mimikatz can be run using:\\n\\n* <code>sekurlsa::Minidump lsassdump.dmp</code>\\n* <code>sekurlsa::logonPasswords</code>\\n\\nBuilt-in Windows tools such as `comsvcs.dll` can also be used:\\n\\n* <code>rundll32.exe C:\\\\Windows\\\\System32\\\\comsvcs.dll MiniDump PID  lsass.dmp full</code>(Citation: Volexity Exchange Marauder March 2021)(Citation: Symantec Attacks Against Government Sector)\\n\\nSimilar to [Image File Execution Options Injection](https://attack.mitre.org/techniques/T1546/012), the silent process exit mechanism can be abused to create a memory dump of `lsass.exe` through Windows Error Reporting (`WerFault.exe`).(Citation: Deep Instinct LSASS)\\n\\nWindows Security Support Provider (SSP) DLLs are loaded into LSASS process at system start. Once loaded into the LSA, SSP DLLs have access to encrypted and plaintext passwords that are stored in Windows, such as any logged-on user's Domain password or smart card PINs. The SSP configuration is stored in two Registry keys: <code>HKLM\\\\SYSTEM\\\\CurrentControlSet\\\\Control\\\\Lsa\\\\Security Packages</code> and <code>HKLM\\\\SYSTEM\\\\CurrentControlSet\\\\Control\\\\Lsa\\\\OSConfig\\\\Security Packages</code>. An adversary may modify these Registry keys to add new SSPs, which will be loaded the next time the system boots, or when the AddSecurityPackage Windows API function is called.(Citation: Graeber 2014)\\n\\nThe following SSPs can be used to access credentials:\\n\\n* Msv: Interactive logons, batch logons, and service logons are done through the MSV authentication package.\\n* Wdigest: The Digest Authentication protocol is designed for use with Hypertext Transfer Protocol (HTTP) and Simple Authentication Security Layer (SASL) exchanges.(Citation: TechNet Blogs Credential Protection)\\n* Kerberos: Preferred for mutual client-server domain authentication in Windows 2000 and later.\\n* CredSSP:  Provides SSO and Network Level Authentication for Remote Desktop Services.(Citation: TechNet Blogs Credential Protection)\\n\",\n",
       " 'tactic': [KillChainPhase(kill_chain_name='mitre-attack', phase_name='credential-access')],\n",
       " 'technique_id': 'T1003.001',\n",
       " 'technique_matrix': ['enterprise-attack'],\n",
       " 'platform': ['Windows'],\n",
       " 'data_sources': ['Process: Process Access',\n",
       "  'Windows Registry: Windows Registry Key Modification',\n",
       "  'Process: Process Creation',\n",
       "  'Process: OS API Execution',\n",
       "  'Logon Session: Logon Session Creation',\n",
       "  'Command: Command Execution',\n",
       "  'File: File Creation']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "techniques_used_by_groups = lift.get_techniques_used_by_all_groups()\n",
    "techniques_used_by_groups[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG BEGINS!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Get Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ATT&CK Groups Markdown Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Creating markadown files for each group..\n",
      "  [>>] Creating markdown file for Indrik Spider..\n",
      "  [>>] Creating markdown file for LuminousMoth..\n",
      "  [>>] Creating markdown file for Wizard Spider..\n",
      "  [>>] Creating markdown file for Elderwood..\n",
      "  [>>] Creating markdown file for FIN7..\n",
      "  [>>] Creating markdown file for WIRTE..\n",
      "  [>>] Creating markdown file for Dragonfly..\n",
      "  [>>] Creating markdown file for OilRig..\n",
      "  [>>] Creating markdown file for Equation..\n",
      "  [>>] Creating markdown file for Fox Kitten..\n",
      "  [>>] Creating markdown file for Lazarus Group..\n",
      "  [>>] Creating markdown file for Aquatic Panda..\n",
      "  [>>] Creating markdown file for TA505..\n",
      "  [>>] Creating markdown file for Inception..\n",
      "  [>>] Creating markdown file for admin@338..\n",
      "  [>>] Creating markdown file for BlackTech..\n",
      "  [>>] Creating markdown file for Malteiro..\n",
      "  [>>] Creating markdown file for Earth Lusca..\n",
      "  [>>] Creating markdown file for Turla..\n",
      "  [>>] Creating markdown file for Suckfly..\n",
      "  [>>] Creating markdown file for TeamTNT..\n",
      "  [>>] Creating markdown file for FIN6..\n",
      "  [>>] Creating markdown file for Silence..\n",
      "  [>>] Creating markdown file for Patchwork..\n",
      "  [>>] Creating markdown file for APT28..\n",
      "  [>>] Creating markdown file for Aoqin Dragon..\n",
      "  [>>] Creating markdown file for Cinnamon Tempest..\n",
      "  [>>] Creating markdown file for HEXANE..\n",
      "  [>>] Creating markdown file for Darkhotel..\n",
      "  [>>] Creating markdown file for Ke3chang..\n",
      "  [>>] Creating markdown file for Leafminer..\n",
      "  [>>] Creating markdown file for Magic Hound..\n",
      "  [>>] Creating markdown file for APT29..\n",
      "  [>>] Creating markdown file for EXOTIC LILY..\n",
      "  [>>] Creating markdown file for Sandworm Team..\n",
      "  [>>] Creating markdown file for Cobalt Group..\n",
      "  [>>] Creating markdown file for Andariel..\n",
      "  [>>] Creating markdown file for HAFNIUM..\n",
      "  [>>] Creating markdown file for APT39..\n",
      "  [>>] Creating markdown file for MuddyWater..\n",
      "  [>>] Creating markdown file for APT38..\n",
      "  [>>] Creating markdown file for Volt Typhoon..\n",
      "  [>>] Creating markdown file for Transparent Tribe..\n",
      "  [>>] Creating markdown file for Ember Bear..\n",
      "  [>>] Creating markdown file for APT32..\n",
      "  [>>] Creating markdown file for BRONZE BUTLER..\n",
      "  [>>] Creating markdown file for POLONIUM..\n",
      "  [>>] Creating markdown file for APT5..\n",
      "  [>>] Creating markdown file for BackdoorDiplomacy..\n",
      "  [>>] Creating markdown file for Kimsuky..\n",
      "  [>>] Creating markdown file for Leviathan..\n",
      "  [>>] Creating markdown file for Ajax Security Team..\n",
      "  [>>] Creating markdown file for Akira..\n",
      "  [>>] Creating markdown file for Mustang Panda..\n",
      "  [>>] Creating markdown file for LAPSUS$..\n",
      "  [>>] Creating markdown file for Chimera..\n",
      "  [>>] Creating markdown file for TA2541..\n",
      "  [>>] Creating markdown file for ToddyCat..\n",
      "  [>>] Creating markdown file for BITTER..\n",
      "  [>>] Creating markdown file for RTM..\n",
      "  [>>] Creating markdown file for menuPass..\n",
      "  [>>] Creating markdown file for Tropic Trooper..\n",
      "  [>>] Creating markdown file for Mustard Tempest..\n",
      "  [>>] Creating markdown file for APT19..\n",
      "  [>>] Creating markdown file for Moses Staff..\n",
      "  [>>] Creating markdown file for Molerats..\n",
      "  [>>] Creating markdown file for Stealth Falcon..\n",
      "  [>>] Creating markdown file for DarkVishnya..\n",
      "  [>>] Creating markdown file for APT37..\n",
      "  [>>] Creating markdown file for Threat Group-1314..\n",
      "  [>>] Creating markdown file for APT41..\n",
      "  [>>] Creating markdown file for FIN13..\n",
      "  [>>] Creating markdown file for Group5..\n",
      "  [>>] Creating markdown file for PLATINUM..\n",
      "  [>>] Creating markdown file for GALLIUM..\n",
      "  [>>] Creating markdown file for FIN10..\n",
      "  [>>] Creating markdown file for Winnti Group..\n",
      "  [>>] Creating markdown file for FIN8..\n",
      "  [>>] Creating markdown file for Rocke..\n",
      "  [>>] Creating markdown file for Scattered Spider..\n",
      "  [>>] Creating markdown file for CURIUM..\n",
      "  [>>] Creating markdown file for Windigo..\n",
      "  [>>] Creating markdown file for Blue Mockingbird..\n",
      "  [>>] Creating markdown file for FIN4..\n",
      "  [>>] Creating markdown file for Gorgon Group..\n",
      "  [>>] Creating markdown file for Sidewinder..\n",
      "  [>>] Creating markdown file for Higaisa..\n",
      "  [>>] Creating markdown file for APT30..\n",
      "  [>>] Creating markdown file for Windshift..\n",
      "  [>>] Creating markdown file for Confucius..\n",
      "  [>>] Creating markdown file for Threat Group-3390..\n",
      "  [>>] Creating markdown file for Tonto Team..\n",
      "  [>>] Creating markdown file for Gamaredon Group..\n",
      "  [>>] Creating markdown file for Rancor..\n",
      "  [>>] Creating markdown file for TA551..\n",
      "  [>>] Creating markdown file for Axiom..\n",
      "  [>>] Creating markdown file for Dark Caracal..\n",
      "  [>>] Creating markdown file for Nomadic Octopus..\n",
      "  [>>] Creating markdown file for APT12..\n",
      "  [>>] Creating markdown file for APT3..\n",
      "  [>>] Creating markdown file for Putter Panda..\n",
      "  [>>] Creating markdown file for Metador..\n",
      "  [>>] Creating markdown file for TA459..\n",
      "  [>>] Creating markdown file for ZIRCONIUM..\n",
      "  [>>] Creating markdown file for APT1..\n",
      "  [>>] Creating markdown file for Naikon..\n",
      "  [>>] Creating markdown file for Sowbug..\n",
      "  [>>] Creating markdown file for Mofang..\n",
      "  [>>] Creating markdown file for Machete..\n",
      "  [>>] Creating markdown file for FIN5..\n",
      "  [>>] Creating markdown file for SideCopy..\n",
      "  [>>] Creating markdown file for APT33..\n",
      "  [>>] Creating markdown file for GOLD SOUTHFIELD..\n",
      "  [>>] Creating markdown file for Volatile Cedar..\n",
      "  [>>] Creating markdown file for Evilnum..\n",
      "  [>>] Creating markdown file for Cleaver..\n",
      "  [>>] Creating markdown file for TEMP.Veles..\n",
      "  [>>] Creating markdown file for DarkHydrus..\n",
      "  [>>] Creating markdown file for Whitefly..\n",
      "  [>>] Creating markdown file for Silent Librarian..\n",
      "  [>>] Creating markdown file for APT18..\n",
      "  [>>] Creating markdown file for Carbanak..\n",
      "  [>>] Creating markdown file for Orangeworm..\n",
      "  [>>] Creating markdown file for Deep Panda..\n",
      "  [>>] Creating markdown file for APT-C-36..\n",
      "  [>>] Creating markdown file for The White Company..\n",
      "  [>>] Creating markdown file for Poseidon Group..\n",
      "  [>>] Creating markdown file for LazyScripter..\n",
      "  [>>] Creating markdown file for Gallmaker..\n",
      "  [>>] Creating markdown file for MoustachedBouncer..\n",
      "  [>>] Creating markdown file for CopyKittens..\n",
      "  [>>] Creating markdown file for Thrip..\n",
      "  [>>] Creating markdown file for PROMETHIUM..\n",
      "  [>>] Creating markdown file for GCMAN..\n",
      "  [>>] Creating markdown file for Ferocious Kitten..\n",
      "  [>>] Creating markdown file for PittyTiger..\n",
      "  [>>] Creating markdown file for APT17..\n",
      "  [>>] Creating markdown file for BlackOasis..\n",
      "  [>>] Creating markdown file for IndigoZebra..\n",
      "  [>>] Creating markdown file for SilverTerrier..\n",
      "  [>>] Creating markdown file for Strider..\n",
      "  [>>] Creating markdown file for Scarlet Mimic..\n",
      "  [>>] Creating markdown file for APT16..\n",
      "  [>>] Creating markdown file for Moafee..\n",
      "  [>>] Creating markdown file for APT-C-23..\n",
      "  [>>] Creating markdown file for Bouncing Golf..\n",
      "  [>>] Creating markdown file for UNC788..\n",
      "  [>>] Creating markdown file for ALLANITE..\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from jinja2 import Template\n",
    "\n",
    "# Create Group docs\n",
    "all_groups = dict()\n",
    "for technique in techniques_used_by_groups:\n",
    "    if technique[\"id\"] not in all_groups:\n",
    "        group = dict()\n",
    "        group[\"group_name\"] = technique[\"name\"]\n",
    "        group[\"group_id\"] = technique[\"external_references\"][0][\"external_id\"]\n",
    "        group[\"created\"] = technique[\"created\"]\n",
    "        group[\"modified\"] = technique[\"modified\"]\n",
    "        group[\"description\"] = technique[\"description\"]\n",
    "        group[\"aliases\"] = technique[\"aliases\"]\n",
    "        if \"x_mitre_contributors\" in technique:\n",
    "            group[\"contributors\"] = technique[\"x_mitre_contributors\"]\n",
    "        group[\"techniques\"] = []\n",
    "        all_groups[technique[\"id\"]] = group\n",
    "    technique_used = dict()\n",
    "    technique_used[\"matrix\"] = technique[\"technique_matrix\"]\n",
    "    technique_used[\"domain\"] = technique[\"x_mitre_domains\"]\n",
    "    technique_used[\"platform\"] = technique[\"platform\"]\n",
    "    technique_used[\"tactics\"] = technique[\"tactic\"]\n",
    "    technique_used[\"technique_id\"] = technique[\"technique_id\"]\n",
    "    technique_used[\"technique_name\"] = technique[\"technique\"]\n",
    "    technique_used[\"use\"] = technique[\"relationship_description\"]\n",
    "    if \"data_sources\" in technique:\n",
    "        technique_used[\"data_sources\"] = technique[\"data_sources\"]\n",
    "    all_groups[technique[\"id\"]][\"techniques\"].append(technique_used)\n",
    "\n",
    "if not os.path.exists(documents_directory):\n",
    "    print(\"[+] Creating knowledge directory..\")\n",
    "    os.makedirs(documents_directory)\n",
    "\n",
    "print(\"[+] Creating markadown files for each group..\")\n",
    "markdown_template = Template(open(group_template).read())\n",
    "for key in list(all_groups.keys()):\n",
    "    group = all_groups[key]\n",
    "    print(\"  [>>] Creating markdown file for {}..\".format(group[\"group_name\"]))\n",
    "    group_for_render = copy.deepcopy(group)\n",
    "    markdown = markdown_template.render(\n",
    "        metadata=group_for_render,\n",
    "        group_name=group[\"group_name\"],\n",
    "        group_id=group[\"group_id\"],\n",
    "    )\n",
    "    file_name = (group[\"group_name\"]).replace(\" \", \"_\")\n",
    "    open(f\"{documents_directory}/{file_name}.md\", encoding=\"utf-8\", mode=\"w\").write(\n",
    "        markdown\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 Index Source Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Loading Group markdown files..\n",
      " [*] Loading admin@338.md\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "unstructured package not found, please install it with `pip install unstructured`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\document_loaders\\unstructured.py:59\u001b[0m, in \u001b[0;36mUnstructuredBaseLoader.__init__\u001b[1;34m(self, mode, post_processors, **unstructured_kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01munstructured\u001b[39;00m  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'unstructured'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m group_files:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m [*] Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(group)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m     loader \u001b[38;5;241m=\u001b[39m \u001b[43mUnstructuredMarkdownLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     md_docs\u001b[38;5;241m.\u001b[39mextend(loader\u001b[38;5;241m.\u001b[39mload())\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[+] Number of .md documents processed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(md_docs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:216\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     emit_warning()\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\document_loaders\\unstructured.py:213\u001b[0m, in \u001b[0;36mUnstructuredFileLoader.__init__\u001b[1;34m(self, file_path, mode, **unstructured_kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize with file path.\"\"\"\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path \u001b[38;5;241m=\u001b[39m file_path\n\u001b[1;32m--> 213\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munstructured_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\document_loaders\\unstructured.py:61\u001b[0m, in \u001b[0;36mUnstructuredBaseLoader.__init__\u001b[1;34m(self, mode, post_processors, **unstructured_kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01munstructured\u001b[39;00m  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munstructured package not found, please install it with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install unstructured`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m     )\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# `single` - elements are combined into one (default)\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# `elements` - maintain individual elements\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# `paged` - elements are combined by page\u001b[39;00m\n\u001b[0;32m     69\u001b[0m _valid_modes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melements\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaged\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n",
      "\u001b[1;31mImportError\u001b[0m: unstructured package not found, please install it with `pip install unstructured`"
     ]
    }
   ],
   "source": [
    "# variables\n",
    "group_files = glob.glob(os.path.join(documents_directory, \"*.md\"))\n",
    "\n",
    "# Loading Markdown files\n",
    "md_docs = []\n",
    "print(\"[+] Loading Group markdown files..\")\n",
    "for group in group_files:\n",
    "    print(f\" [*] Loading {os.path.basename(group)}\")\n",
    "    loader = UnstructuredMarkdownLoader(group)\n",
    "    md_docs.extend(loader.load())\n",
    "\n",
    "print(f\"[+] Number of .md documents processed: {len(md_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check a doc page content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(md_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Split Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use langchain text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursively split by character\n",
    "# This text splitter is the recommended one for generic text.\n",
    "# It is parameterized by a list of characters. It tries to split on them in\n",
    "# order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"].\n",
    "# This has the effect of trying to keep all paragraphs (and then sentences, and then words)\n",
    "# together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(\n",
    "        text, disallowed_special=()  # To disable this check for all special tokens\n",
    "    )\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking Text\n",
    "print(\"[+] Initializing RecursiveCharacterTextSplitter..\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,  # number of tokens overlap between chunks\n",
    "    add_start_index=True,  # the character index at which each split Document starts within the initial Document is preserved\n",
    "    length_function=tiktoken_len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[+] Splitting documents in chunks..\")\n",
    "chunks = text_splitter.split_documents(md_docs)\n",
    "\n",
    "print(f\"[+] Number of documents: {len(md_docs)}\")\n",
    "print(f\"[+] Number of chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunks[0])\n",
    "print(chunks[1])\n",
    "print(chunks[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Embed Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS  # , DistanceStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the embeddings function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to define the OpenAI embeddings function\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# If you want to define an open-source embedding function\n",
    "embeddings_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "# Equivalent to SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create or load database from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/v0.2/docs/integrations/vectorstores/faiss/\n",
    "from langchain_community.vectorstores.faiss import DistanceStrategy\n",
    "\n",
    "db_dir = \"data/faiss/faiss_index\"\n",
    "\n",
    "# Check if database directory exists\n",
    "if os.path.exists(db_dir):\n",
    "    # Load database from disk\n",
    "    db = FAISS.load_local(\n",
    "        folder_path=db_dir,\n",
    "        embeddings=embeddings_function,\n",
    "        distance_strategy=DistanceStrategy.COSINE,\n",
    "        allow_dangerous_deserialization=True,\n",
    "    )\n",
    "else:\n",
    "    # With OpenAI Embeddings\n",
    "    # db = FAISS.from_documents(chunks, OpenAIEmbeddings())\n",
    "\n",
    "    # Create a new database\n",
    "    db = FAISS.from_documents(\n",
    "        chunks, embedding=embeddings_function, distance_strategy=DistanceStrategy.COSINE\n",
    "    )\n",
    "    # Sabe database to disk\n",
    "    db.save_local(\"data/faiss/faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ask a question directly to the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query it\n",
    "query = \"What threat actor sends text messages over social media to their targets?\"\n",
    "relevant_docs = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "print(relevant_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Enable Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Database as a Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Initialze LLM Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo-0125\", openai_api_key=openai_api_key, temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporate the Retriever into a Question-Answering chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a Threat Intelligence assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05. Initialize Conversation with Context / Relevant Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What threat actor sends text messages over social media to their targets?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke({\"input\": query})\n",
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
