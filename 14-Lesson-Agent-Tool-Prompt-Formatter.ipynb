{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8: 14 - Anatomy of an AI Agent - Enabling Prompt Templates\n",
    "---------------------------------------------------------------------------------\n",
    "In this lesson, we will support LLMs that do not have function calling capabilities by using a basic prompt. We will pass the tools and their details, such as function signatures, to the LLM. This will allow the LLM to output a structured response containing the tool details in JSON format, similar to how OpenAI Function Calling works.\n",
    "\n",
    "## Objectives\n",
    "* Support LLMs that do not have built-in function calling capabilities.\n",
    "* Use prompts to pass tool details and function signatures to the LLM.\n",
    "* Generate structured responses with tool details in JSON format.\n",
    "* Ensure seamless tool execution by interpreting the LLM's structured responses.\n",
    "\n",
    "## What this session covers:\n",
    "* Defining the current agent structure, including the LLM Client and short-term memory.\n",
    "* Creating prompts to pass tool details and function signatures to the LLM.\n",
    "* Interpreting the LLM's structured responses for tool execution.\n",
    "* Implementing a mechanism to parse and validate the LLM's output.\n",
    "* Integrating and testing the enhanced Agent with LLMs that lack function calling capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Current Agent Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List\n",
    "import openai\n",
    "\n",
    "class OpenAIChatCompletion:\n",
    "    \"\"\"Interacts with OpenAI's API for chat completions.\"\"\"\n",
    "    def __init__(self, model: str, api_key: str = None, base_url: str = None):\n",
    "        self.client = openai.OpenAI(api_key=api_key, base_url=base_url)\n",
    "        self.model = model\n",
    "\n",
    "    def generate(self, messages: List[str], tools: List[Dict[str, Any]] = None, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Generates a response from OpenAI's API.\"\"\"\n",
    "        params = {'messages': messages, 'model': self.model, 'tools': tools, **kwargs}\n",
    "        response = self.client.chat.completions.create(**params)\n",
    "        return response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short-Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "class ChatMessageMemory:\n",
    "    \"\"\"Manages conversation context.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "    \n",
    "    def add_message(self, message: Dict):\n",
    "        \"\"\"Add a message to memory.\"\"\"\n",
    "        self.messages.append(message)\n",
    "    \n",
    "    def add_messages(self, messages: List[Dict]):\n",
    "        \"\"\"Add multiple messages to memory.\"\"\"\n",
    "        for message in messages:\n",
    "            self.add_message(message)\n",
    "    \n",
    "    def get_messages(self) -> List[Dict]:\n",
    "        \"\"\"Retrieve all messages.\"\"\"\n",
    "        return self.messages.copy()\n",
    "    \n",
    "    def reset_memory(self):\n",
    "        \"\"\"Clear all messages.\"\"\"\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, ValidationError\n",
    "from typing import Callable, Type\n",
    "from inspect import signature\n",
    "\n",
    "class AgentTool:\n",
    "    \"\"\"Encapsulates a Python function with Pydantic validation.\"\"\"\n",
    "    def __init__(self, func: Callable, args_model: Type[BaseModel]):\n",
    "        self.func = func\n",
    "        self.args_model = args_model\n",
    "        self.name = func.__name__\n",
    "        self.description = func.__doc__ or self.args_schema.get('description', '')\n",
    "\n",
    "    def to_openai_function_call_definition(self) -> dict:\n",
    "        \"\"\"Converts the tool to OpenAI Function Calling format.\"\"\"\n",
    "        schema_dict = self.args_schema\n",
    "        description = schema_dict.pop(\"description\", \"\")\n",
    "        return {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": self.name,\n",
    "                \"description\": description,\n",
    "                \"parameters\": schema_dict\n",
    "            }\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def args_schema(self) -> dict:\n",
    "        \"\"\"Returns the tool's function argument schema as a dictionary.\"\"\"\n",
    "        schema = self.args_model.model_json_schema()\n",
    "        schema.pop(\"title\", None)\n",
    "        return schema\n",
    "\n",
    "    def validate_json_args(self, json_string: str) -> bool:\n",
    "        \"\"\"Validate JSON string using the Pydantic model.\"\"\"\n",
    "        try:\n",
    "            validated_args = self.args_model.model_validate_json(json_string)\n",
    "            return isinstance(validated_args, self.args_model)\n",
    "        except ValidationError:\n",
    "            return False\n",
    "\n",
    "    def run(self, *args, **kwargs) -> Any:\n",
    "        \"\"\"Execute the function with validated arguments.\"\"\"\n",
    "        try:\n",
    "            # Handle positional arguments by converting them to keyword arguments\n",
    "            if args:\n",
    "                sig = signature(self.func)\n",
    "                arg_names = list(sig.parameters.keys())\n",
    "                kwargs.update(dict(zip(arg_names, args)))\n",
    "\n",
    "            # Validate arguments with the provided Pydantic schema\n",
    "            validated_args = self.args_model(**kwargs)\n",
    "            return self.func(**validated_args.model_dump())\n",
    "        except ValidationError as e:\n",
    "            raise ValueError(f\"Argument validation failed for tool '{self.name}': {str(e)}\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"An error occurred during the execution of tool '{self.name}': {str(e)}\")\n",
    "\n",
    "    def __call__(self, *args, **kwargs) -> Any:\n",
    "        \"\"\"Allow the AgentTool instance to be called like a regular function.\"\"\"\n",
    "        return self.run(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Tool Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional, Type\n",
    "from pydantic import BaseModel\n",
    "\n",
    "def check_docstring(func: Callable):\n",
    "    \"\"\"Ensure the function has a docstring.\"\"\"\n",
    "    if not func.__doc__:\n",
    "        raise ValueError(f\"Function '{func.__name__}' must have a docstring.\")\n",
    "\n",
    "def Tool(func: Optional[Callable] = None, *, args_model: Type[BaseModel]) -> AgentTool:\n",
    "    \"\"\"Decorator to wrap a function with an AgentTool instance.\"\"\"\n",
    "    def decorator(f: Callable) -> AgentTool:\n",
    "        check_docstring(f)\n",
    "        return AgentTool(f, args_model=args_model)\n",
    "    return decorator(func) if func else decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Tool Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional\n",
    "import json\n",
    "\n",
    "class AgentToolExecutor:\n",
    "    \"\"\"Manages tool registration and execution.\"\"\"\n",
    "    \n",
    "    def __init__(self, tools: Optional[List[AgentTool]] = None):\n",
    "        self.tools: Dict[str, AgentTool] = {}\n",
    "        if tools:\n",
    "            for tool in tools:\n",
    "                self.register_tool(tool)\n",
    "    \n",
    "    def register_tool(self, tool: AgentTool):\n",
    "        \"\"\"Registers a tool.\"\"\"\n",
    "        if tool.name in self.tools:\n",
    "            raise ValueError(f\"Tool '{tool.name}' is already registered.\")\n",
    "        self.tools[tool.name] = tool\n",
    "      \n",
    "    def execute(self, tool_name: str, tool_args: str) -> Any:\n",
    "        \"\"\"Executes a tool by name with given arguments.\"\"\"\n",
    "        print(f\"Checking if {tool_name} exists..\")\n",
    "        tool = self.tools.get(tool_name)\n",
    "        if not tool:\n",
    "            raise ValueError(f\"Tool '{tool_name}' not found.\")\n",
    "        try:\n",
    "            print(f\"Validating {tool_name} suggested args {tool_args}\")\n",
    "            if tool.validate_json_args(tool_args):\n",
    "                tool_args_dict = json.loads(tool_args)\n",
    "                print(f\"Executing {tool_name} with args: {tool_args}\")\n",
    "                return tool.run(**tool_args_dict)\n",
    "            else:\n",
    "                raise ValueError(f\"Error validating tool '{tool_name}' arguments.\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error executing tool '{tool_name}': {e}\") from e\n",
    "    \n",
    "    def get_tool_names(self) -> List[str]:\n",
    "        \"\"\"Returns a list of all registered tool names.\"\"\"\n",
    "        return list(self.tools.keys())\n",
    "    \n",
    "    def get_tool_details(self) -> str:\n",
    "        \"\"\"Returns details of all registered tools.\"\"\"\n",
    "        tools_info = [f\"{tool.name}: {tool.description} Args schema: {tool.args_schema['properties']}\" for tool in self.tools.values()]\n",
    "        return '\\n'.join(tools_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Tool Calling Agent Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Integrates LLM client, tools, memory, and manages tool executions.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client, system_message: Dict[str, str], max_iterations: int = 10, tools: Optional[List[AgentTool]] = None):\n",
    "        self.llm_client = llm_client\n",
    "        self.executor = AgentToolExecutor()\n",
    "        self.memory = ChatMessageMemory()\n",
    "        self.system_message = system_message\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tool_history = []\n",
    "        self.function_calls = None\n",
    "        \n",
    "        # Register and convert tools\n",
    "        if tools:\n",
    "            for tool in tools:\n",
    "                self.executor.register_tool(tool)\n",
    "            self.function_calls = [tool.to_openai_function_call_definition() for tool in tools]\n",
    "\n",
    "    def run(self, user_message: Dict[str, str]):\n",
    "        \"\"\"Generates responses, manages tool calls, and updates memory.\"\"\"\n",
    "        self.memory.add_message(user_message)\n",
    "\n",
    "        for _ in range(self.max_iterations):\n",
    "            chat_history = [self.system_message] + self.memory.get_messages() + self.tool_history\n",
    "            print(f\"chat history sent: {chat_history}\")\n",
    "            response = self.llm_client.generate(chat_history, tools=self.function_calls)\n",
    "\n",
    "            if self.parse_response(response):\n",
    "                continue\n",
    "            else:\n",
    "                self.memory.add_message(response)\n",
    "                self.tool_history = []\n",
    "                return response\n",
    "\n",
    "    def parse_response(self, response) -> bool:\n",
    "        \"\"\"Executes tool calls suggested by the LLM and updates tool history.\"\"\"\n",
    "        if response.tool_calls:\n",
    "            self.tool_history.append(response)\n",
    "            for tool in response.tool_calls:\n",
    "                tool_name = tool.function.name\n",
    "                tool_args = tool.function.arguments\n",
    "                try:\n",
    "                    execution_results = self.executor.execute(tool_name, tool_args)\n",
    "                    self.tool_history.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": tool.id,\n",
    "                        \"name\": tool_name,\n",
    "                        \"content\": str(execution_results)\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    raise ValueError(f\"Execution error in tool '{tool_name}': {e}\") from e\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enabling Prompt Formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class StringPromptTemplate:\n",
    "    \"\"\"Handles dynamic prompt formatting for an AI agent.\"\"\"\n",
    "\n",
    "    def __init__(self, template: str):\n",
    "        \"\"\"Initializes the prompt template and extracts variables.\"\"\"\n",
    "        self.template = template\n",
    "        self.variables = {}\n",
    "        self.required_variables = self.extract_variables()\n",
    "\n",
    "    def extract_variables(self):\n",
    "        \"\"\"Extracts placeholders from the template.\"\"\"\n",
    "        return set(re.findall(r'\\{(.*?)\\}', self.template))\n",
    "\n",
    "    def update_variables(self, **kwargs):\n",
    "        \"\"\"Updates template variables.\"\"\"\n",
    "        self.variables.update(kwargs)\n",
    "        self.required_variables -= set(kwargs.keys())\n",
    "\n",
    "    def format_prompt(self, **kwargs):\n",
    "        \"\"\"Generates a formatted prompt and tracks remaining variables.\"\"\"\n",
    "        combined_variables = {**self.variables, **kwargs}\n",
    "        self.required_variables -= set(kwargs.keys())\n",
    "        return self.template.format(**combined_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Prompt Formatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize String Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_PROMPT = \"\"\"\n",
    "You have access to the following tools:\n",
    "\n",
    "{tool_details}\n",
    "\n",
    "Respond to the user with the right tool and input whenever is needed.\n",
    "When responding to the user, provide only ONE tool per $JSON_BLOB, as shown in the example below delimited by triple backticks:\n",
    "\n",
    "```\n",
    "{{\n",
    "    \"name\": $TOOL_NAME,\n",
    "    \"arguments\": $INPUT\n",
    "}}\n",
    "```\n",
    "\n",
    "User input: What is the weather in Virginia?\n",
    "\"\"\"\n",
    "formatter = StringPromptTemplate(USER_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "import random\n",
    "\n",
    "class GetWeatherSchema(BaseModel):\n",
    "    \"\"\"Get weather information based on location.\"\"\"\n",
    "    location: str = Field(description=\"Location to get weather for\")\n",
    "\n",
    "@Tool(args_model=GetWeatherSchema)\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Gets weather information.\"\"\"\n",
    "    temperature = random.randint(60, 80)\n",
    "    return f\"{location}: {temperature}F.\"\n",
    "\n",
    "class JumpSchema(BaseModel):\n",
    "    \"\"\"Jump a specific distance\"\"\"\n",
    "    distance: str = Field(description=\"Specific distance to jump for\")\n",
    "\n",
    "@Tool(args_model=JumpSchema)\n",
    "def jump(distance: str) -> str:\n",
    "    \"\"\"Jumps a specific distance.\"\"\"\n",
    "    return f\"I jumped the following distance {distance}\"\n",
    "\n",
    "tools = [get_weather, jump]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You have access to the following tools:\n",
      "\n",
      "get_weather: Gets weather information. Args schema: {'location': {'description': 'Location to get weather for', 'title': 'Location', 'type': 'string'}}\n",
      "jump: Jumps a specific distance. Args schema: {'distance': {'description': 'Specific distance to jump for', 'title': 'Distance', 'type': 'string'}}\n",
      "\n",
      "Respond to the user with the right tool and input whenever is needed.\n",
      "When responding to the user, provide only ONE tool per $JSON_BLOB, as shown in the example below delimited by triple backticks:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"name\": $TOOL_NAME,\n",
      "    \"arguments\": $INPUT\n",
      "}\n",
      "```\n",
      "\n",
      "User input: What is the weather in Virginia?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get tool details\n",
    "tools_info = [f\"{tool.name}: {tool.description} Args schema: {tool.args_schema['properties']}\" for tool in tools]\n",
    "tool_details = '\\n'.join(tools_info)\n",
    "\n",
    "user_prompt_formatted = formatter.format_prompt(tool_details=tool_details)\n",
    "print(user_prompt_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API from environment variable\n",
    "# import os\n",
    "# api_key = os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "api_key=\"\"\n",
    "\n",
    "client = OpenAIChatCompletion(\n",
    "    base_url='https://api.openai.com/v1',\n",
    "    model='gpt-4o',\n",
    "    api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define System messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system message\n",
    "system_message = {\"role\": \"system\", \"content\": \"You are a weather assistant.\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Agent with the LLM client and system message\n",
    "agent = Agent(llm_client=client, system_message=system_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send a User Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat history sent: [{'role': 'system', 'content': 'You are a weather assistant.'}, {'role': 'user', 'content': '\\nYou have access to the following tools:\\n\\nget_weather: Gets weather information. Args schema: {\\'location\\': {\\'description\\': \\'Location to get weather for\\', \\'title\\': \\'Location\\', \\'type\\': \\'string\\'}}\\njump: Jumps a specific distance. Args schema: {\\'distance\\': {\\'description\\': \\'Specific distance to jump for\\', \\'title\\': \\'Distance\\', \\'type\\': \\'string\\'}}\\n\\nRespond to the user with the right tool and input whenever is needed.\\nWhen responding to the user, provide only ONE tool per $JSON_BLOB, as shown in the example below delimited by triple backticks:\\n\\n```\\n{\\n    \"name\": $TOOL_NAME,\\n    \"arguments\": $INPUT\\n}\\n```\\n\\nUser input: What is the weather in Virginia?\\n'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content='```\\n{\\n    \"name\": \"get_weather\",\\n    \"arguments\": {\\n        \"location\": \"Virginia\"\\n    }\\n}\\n```', role='assistant', function_call=None, tool_calls=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a response using the agent\n",
    "user_message = {\"role\": \"user\", \"content\": user_prompt_formatted}\n",
    "\n",
    "response = agent.run(user_message)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "import json\n",
    "\n",
    "def parse_nested_json(text):\n",
    "    # Unescape backslashes\n",
    "    text = text.replace('\\\\\\\\n', '\\\\n').replace('\\\\n', '\\n').replace('\\\\\\'', '\\'').replace('\\\\\\\\', '\\\\')\n",
    "    # Replace double curly braces with single curly braces\n",
    "    text = text.replace('{{', '{').replace('}}', '}')\n",
    "    pattern = regex.compile(r'\\{(?:[^{}]|(?R))*\\}') # Supports nested structures\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group())\n",
    "        except json.JSONDecodeError:  # Corrected to use json.JSONDecodeError\n",
    "            pass\n",
    "    return None  # No valid JSON found or parsing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'get_weather', 'arguments': {'location': 'Virginia'}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_nested_json(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Agent Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Integrates key components and manages tool executions.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client, system_message: Dict[str, str], max_iterations: int = 10, tools: Optional[List[AgentTool]] = None, prompt_template: StringPromptTemplate = None):\n",
    "        self.llm_client = llm_client\n",
    "        self.executor = AgentToolExecutor()\n",
    "        self.memory = ChatMessageMemory()\n",
    "        self.system_message = system_message\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tool_history = []\n",
    "        self.function_calls = None\n",
    "        self.prompt_template = prompt_template\n",
    "\n",
    "        if tools:\n",
    "            for tool in tools:\n",
    "                self.executor.register_tool(tool)\n",
    "            self.function_calls = [tool.to_openai_function_call_definition() for tool in tools]\n",
    "\n",
    "        tool_details = self.executor.get_tool_details()\n",
    "        tool_names = ' or '.join(self.executor.get_tool_names())\n",
    "        self.prompt_template.update_variables(\n",
    "            system_message=self.system_message,\n",
    "            tool_details=tool_details,\n",
    "            tool_names=tool_names\n",
    "        )\n",
    "\n",
    "    def run(self, task: str):\n",
    "        \"\"\"Generates responses, manages tool calls, and updates memory.\"\"\"\n",
    "        self.memory.add_message({\"role\": \"user\", \"content\": task})\n",
    "\n",
    "        for _ in range(self.max_iterations):\n",
    "            chat_history = self.messages_to_string()\n",
    "            formatted_message = self.prompt_template.format_prompt(chat_history=chat_history, user_input=task)\n",
    "            messages = [{\"role\": \"user\", \"content\": formatted_message}]\n",
    "            response = self.llm_client.generate(messages=messages)\n",
    "            action_dict = self.parse_response(response)\n",
    "\n",
    "            if action_dict:\n",
    "                action_name = action_dict[\"name\"]\n",
    "                action_arguments = action_dict[\"args_json\"]\n",
    "                execution_results = self.executor.execute(action_name, action_arguments)\n",
    "                return execution_results\n",
    "            else:\n",
    "                logger.info(\"Agent is responding directly.\")\n",
    "                self.memory.add_conversation(user_message={\"role\": \"user\", \"content\": task}, assistant_message=response)\n",
    "                return response\n",
    "\n",
    "    def parse_response(self, response: Dict):\n",
    "        \"\"\"Extracts tools or continues the conversation.\"\"\"\n",
    "        import regex\n",
    "\n",
    "        pattern = regex.compile(r'\\{(?:[^{}]|(?R))*\\}')  # Supports nested structures\n",
    "        message_content = response.content\n",
    "        # Unescape backslashes\n",
    "        message_content = message_content.replace('\\\\\\\\n', '\\\\n').replace('\\\\n', '\\n').replace('\\\\\\'', '\\'').replace('\\\\\\\\', '\\\\')\n",
    "        # Replace double curly braces with single curly braces\n",
    "        message_content = message_content.replace('{{', '{').replace('}}', '}')\n",
    "\n",
    "        match = pattern.search(message_content)\n",
    "        if match:\n",
    "            action_content = match.group()\n",
    "            try:\n",
    "                action_dict = json.loads(action_content.strip())\n",
    "                action_dict['args_json'] = json.dumps(action_dict[\"arguments\"])\n",
    "                return action_dict\n",
    "            except json.JSONDecodeError:\n",
    "                raise ValueError(\"Invalid JSON in action content\")\n",
    "        return None\n",
    "    \n",
    "    def messages_to_string(self) -> str:\n",
    "        \"\"\"Converts messages to a string.\"\"\"\n",
    "        formatted_messages = []\n",
    "        for message in self.memory.get_messages():\n",
    "            formatted_messages.append(f\"{message['role'].capitalize()}: {message['content']}\")\n",
    "        return \"\\n\".join(formatted_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test New Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API from environment variable\n",
    "# import os\n",
    "# api_key = os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "api_key=\"\"\n",
    "\n",
    "client = OpenAIChatCompletion(\n",
    "    base_url='https://api.openai.com/v1',\n",
    "    model='gpt-4o',\n",
    "    api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define System messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system message\n",
    "system_message = {\"role\": \"system\", \"content\": \"You are a weather assistant.\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRING_PROMPT_TEMPLATE = \"\"\"\n",
    "{system_message}. You have access to the following tools:\n",
    "\n",
    "{tool_details}\n",
    "\n",
    "Respond to the the user with the right tool and input whenever is needed.\n",
    "\n",
    "Valid tool name values: {tool_names}.\n",
    "\n",
    "When responding to the user, provide only ONE tool per $JSON_BLOB, as shown in the example below delimited by triple backticks:\n",
    "\n",
    "```\n",
    "{{\n",
    "    \"name\": $TOOL_NAME,\n",
    "    \"arguments\": $INPUT\n",
    "}}\n",
    "```\n",
    "\n",
    "Previous conversation history:\n",
    "{chat_history}\n",
    "\n",
    "New conversation:\n",
    "{user_input}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = StringPromptTemplate(STRING_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Agent with the LLM client and system message\n",
    "agent = Agent(llm_client=client, system_message=system_message, tools=tools, prompt_template=prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run User Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if get_weather exists..\n",
      "Validating get_weather suggested args {\"location\": \"New York\"}\n",
      "Executing get_weather with args: {\"location\": \"New York\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'New York: 76F.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"What is the weather in New York?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
