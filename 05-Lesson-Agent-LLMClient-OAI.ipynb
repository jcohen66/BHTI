{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8: 5 - Anatomy of an AI Agent - OpenAI LLM Client\n",
    "----------------------------------------------------------------------------\n",
    "In this lesson, we will demonstrate the process of creating an AI Agent by defining a base Agent class and integrating it with an LLM Client. We will use OpenAI models, with the LLM Client serving as our proxy gateway to OpenAI's powerful language models. Finally, we will assemble the Agent with the LLM Client and test it to showcase its functionality.\n",
    "\n",
    "## Objectives\n",
    "* Understand the structure and components of a base Agent class.\n",
    "* Learn how to define and integrate an LLM Client using OpenAI models.\n",
    "* Assemble the AI Agent and test its ability to generate responses.\n",
    "\n",
    "## What this session covers:\n",
    "* Installing necessary libraries for OpenAI integration.\n",
    "* Defining the base structure of the Agent class.\n",
    "* Implementing the LLM Client for interaction with OpenAI's Chat Completion API.\n",
    "* Integrating the LLM Client with the Agent.\n",
    "* Initializing and testing the Agent with example messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agent Base Structure\n",
    "\n",
    "This foundational class was designed to be flexible and extensible, serving as a template for integrating more complex features as development progressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    Integrates LLM client, tools, and memory.\n",
    "    \"\"\"\n",
    "    def __init__(self, llm_client=None, tools=None, memory=None):\n",
    "        self.llm_client = llm_client\n",
    "        self.tools = tools\n",
    "        self.memory = memory\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Placeholder for the agent's main execution logic.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LLM Client\n",
    "\n",
    "This class interacts with the [OpenAI Chat completion API](https://platform.openai.com/docs/guides/text-generation/chat-completions-api?ref=blog.openthreatresearch.com), sending messages and parsing responses to handle various message types effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List\n",
    "import openai\n",
    "\n",
    "class OpenAIChatCompletion:\n",
    "    \"\"\"\n",
    "    Interacts with OpenAI's API for chat completions.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: str, api_key: str = None, base_url: str = None):\n",
    "        \"\"\"\n",
    "        Initialize with model, API key, and base URL.\n",
    "        \"\"\"\n",
    "        self.client = openai.OpenAI(api_key=api_key, base_url=base_url)\n",
    "        self.model = model\n",
    "\n",
    "    def generate(self, messages: List[Dict], **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate a response from input messages.\n",
    "        \"\"\"\n",
    "        params = {'messages': messages, 'model': self.model, **kwargs}\n",
    "        response = self.client.chat.completions.create(**params)\n",
    "        return response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating LLM Client with AI Agent\n",
    "\n",
    "This enables the agent to leverage the LLM client for generating responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    Integrates LLM client, tools, and memory.\n",
    "    \"\"\"\n",
    "    def __init__(self, llm_client, tools=None, memory=None):\n",
    "        self.llm_client = llm_client\n",
    "        self.tools = tools\n",
    "        self.memory = memory\n",
    "\n",
    "    def run(self, messages: List[Dict[str, str]]):\n",
    "        \"\"\"\n",
    "        Generates a response from the LLM client.\n",
    "        \"\"\"\n",
    "        # Generate response using the LLM client\n",
    "        response = self.llm_client.generate(messages)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing LLM Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API from environment variable\n",
    "# import os\n",
    "# api_key = os.getenv(\"OPENAI_API_KEY\"))\n",
    "import os\n",
    "api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAIChatCompletion(\n",
    "    base_url='https://api.openai.com/v1',\n",
    "    model='gpt-4',\n",
    "    api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a security assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hey! This is Roberto!\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add LLM client to Agent and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent with the LLM client\n",
    "myAgent = Agent(llm_client=client)\n",
    "\n",
    "# Use the agent to run a task\n",
    "response = myAgent.run(messages=messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.to_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
