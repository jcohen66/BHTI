13:31:47,219 graphrag.config.read_dotenv INFO Loading pipeline .env file
13:31:47,222 graphrag.index.cli INFO using default configuration: {
    "llm": {
        "api_key": "REDACTED, length 51",
        "type": "openai_chat",
        "model": "gpt-4-turbo-preview",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "./data",
    "reporting": {
        "type": "file",
        "base_dir": "output/${timestamp}/reports",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "output/${timestamp}/artifacts",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "documents",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.md$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_chat",
            "model": "gpt-4-turbo-preview",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_chat",
            "model": "gpt-4-turbo-preview",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_chat",
            "model": "gpt-4-turbo-preview",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_chat",
            "model": "gpt-4-turbo-preview",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
13:31:47,223 graphrag.index.create_pipeline_config INFO skipping workflows 
13:31:47,248 graphrag.index.run INFO Running pipeline
13:31:47,248 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at data/output/20240801-133147/artifacts
13:31:47,249 graphrag.index.input.load_input INFO loading input from root_dir=documents
13:31:47,249 graphrag.index.input.load_input INFO using file storage for input
13:31:47,250 graphrag.index.storage.file_pipeline_storage INFO search data/documents for files matching .*\.md$
13:31:47,250 graphrag.index.input.text INFO found text files from documents, found [('FIN7.md', {}), ('Sandworm_Team.md', {}), ('Carbanak.md', {}), ('Turla.md', {}), ('APT29.md', {}), ('APT3.md', {}), ('Wizard_Spider.md', {}), ('menuPass.md', {}), ('Scattered_Spider.md', {})]
13:31:47,255 graphrag.index.input.text INFO Found 9 files, loading 9
13:31:47,257 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'join_text_units_to_entity_ids', 'create_final_relationships', 'join_text_units_to_relationship_ids', 'create_final_community_reports', 'create_final_text_units', 'create_base_documents', 'create_final_documents']
13:31:47,257 graphrag.index.run INFO Final # of rows loaded: 9
13:31:47,386 graphrag.index.run INFO Running workflow: create_base_text_units...
13:31:47,386 graphrag.index.run INFO dependencies for create_base_text_units: []
13:31:47,389 datashaper.workflow.workflow INFO executing verb orderby
13:31:47,395 datashaper.workflow.workflow INFO executing verb zip
13:31:47,397 datashaper.workflow.workflow INFO executing verb aggregate_override
13:31:47,402 datashaper.workflow.workflow INFO executing verb chunk
13:31:47,561 datashaper.workflow.workflow INFO executing verb select
13:31:47,565 datashaper.workflow.workflow INFO executing verb unroll
13:31:47,570 datashaper.workflow.workflow INFO executing verb rename
13:31:47,572 datashaper.workflow.workflow INFO executing verb genid
13:31:47,576 datashaper.workflow.workflow INFO executing verb unzip
13:31:47,579 datashaper.workflow.workflow INFO executing verb copy
13:31:47,582 datashaper.workflow.workflow INFO executing verb filter
13:31:47,590 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
13:31:47,751 graphrag.index.run INFO Running workflow: create_base_extracted_entities...
13:31:47,752 graphrag.index.run INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
13:31:47,752 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
13:31:47,780 datashaper.workflow.workflow INFO executing verb entity_extract
13:31:47,784 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
13:31:47,808 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4-turbo-preview: TPM=0, RPM=0
13:31:47,808 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4-turbo-preview: 25
13:31:58,461 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:31:58,466 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.589999999850988. input_tokens=2746, output_tokens=261
13:32:02,24 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:02,26 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 14.18999999994412. input_tokens=3135, output_tokens=289
13:32:03,665 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:03,668 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 15.824000000022352. input_tokens=3134, output_tokens=348
13:32:04,724 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:04,726 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 16.885000000009313. input_tokens=3135, output_tokens=354
13:32:06,20 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:06,24 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 18.165000000037253. input_tokens=2398, output_tokens=329
13:32:06,130 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:06,133 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 18.260999999940395. input_tokens=3134, output_tokens=400
13:32:08,792 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:08,795 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 20.907000000122935. input_tokens=2279, output_tokens=460
13:32:08,990 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:08,993 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 21.114999999990687. input_tokens=3134, output_tokens=389
13:32:11,834 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:11,835 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 23.97400000016205. input_tokens=3134, output_tokens=508
13:32:11,844 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:11,845 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 23.999000000068918. input_tokens=2306, output_tokens=583
13:32:12,17 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:12,21 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 24.139999999897555. input_tokens=3135, output_tokens=581
13:32:12,646 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:12,648 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 24.756999999983236. input_tokens=3134, output_tokens=470
13:32:13,150 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:13,153 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 25.282999999821186. input_tokens=3134, output_tokens=462
13:32:14,300 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:14,302 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.471000000135973. input_tokens=3133, output_tokens=594
13:32:15,140 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:15,142 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.246999999973923. input_tokens=3134, output_tokens=469
13:32:17,93 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:17,96 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.256999999983236. input_tokens=3135, output_tokens=667
13:32:17,410 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:17,412 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.527999999932945. input_tokens=3134, output_tokens=538
13:32:18,31 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:18,35 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 30.201000000117347. input_tokens=3134, output_tokens=559
13:32:19,56 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:19,59 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.205000000074506. input_tokens=3134, output_tokens=670
13:32:21,196 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:21,200 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 16.471999999834225. input_tokens=3133, output_tokens=361
13:32:21,301 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:21,304 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 19.27600000007078. input_tokens=3134, output_tokens=425
13:32:21,817 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:21,819 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 33.954000000143424. input_tokens=3133, output_tokens=645
13:32:22,721 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:22,723 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 34.898999999975786. input_tokens=3134, output_tokens=850
13:32:25,102 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:25,103 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 13.256999999983236. input_tokens=2226, output_tokens=288
13:32:26,197 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:26,201 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.73300000000745. input_tokens=3134, output_tokens=608
13:32:27,328 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:27,330 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 15.307000000029802. input_tokens=2917, output_tokens=322
13:32:27,583 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:27,587 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 23.916999999899417. input_tokens=2278, output_tokens=442
13:32:28,585 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:28,589 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 40.73699999996461. input_tokens=3134, output_tokens=974
13:32:30,970 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:30,975 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 8.250999999931082. input_tokens=19, output_tokens=205
13:32:31,957 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:31,960 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 17.655999999959022. input_tokens=3134, output_tokens=397
13:32:33,63 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:33,67 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.932000000029802. input_tokens=3134, output_tokens=726
13:32:33,258 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:33,262 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.236000000033528. input_tokens=3135, output_tokens=542
13:32:33,792 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:33,795 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 45.92599999997765. input_tokens=3135, output_tokens=852
13:32:33,806 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:33,807 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.476000000024214. input_tokens=19, output_tokens=147
13:32:34,714 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:34,718 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 46.86100000003353. input_tokens=3134, output_tokens=1077
13:32:34,750 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:34,752 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 19.608999999938533. input_tokens=3135, output_tokens=454
13:32:35,486 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:35,490 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.692999999970198. input_tokens=3135, output_tokens=628
13:32:36,743 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:36,745 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 14.924000000115484. input_tokens=19, output_tokens=290
13:32:38,61 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:38,63 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 9.472000000067055. input_tokens=19, output_tokens=260
13:32:38,333 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:38,336 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 12.134000000078231. input_tokens=19, output_tokens=259
13:32:38,343 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:38,345 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 13.239999999990687. input_tokens=19, output_tokens=302
13:32:39,917 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:39,920 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 52.03300000005402. input_tokens=3134, output_tokens=1023
13:32:40,224 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:40,227 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 28.389999999897555. input_tokens=3134, output_tokens=598
13:32:40,853 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:40,857 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 22.820999999996275. input_tokens=3134, output_tokens=565
13:32:41,707 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:41,711 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 32.71499999985099. input_tokens=3134, output_tokens=816
13:32:43,233 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:43,236 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.13800000003539. input_tokens=3134, output_tokens=707
13:32:44,710 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:44,714 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 23.40900000021793. input_tokens=2099, output_tokens=477
13:32:46,908 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:46,912 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 13.104000000050291. input_tokens=19, output_tokens=260
13:32:48,400 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:48,402 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 8.174999999813735. input_tokens=19, output_tokens=198
13:32:49,556 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:49,560 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 30.498000000137836. input_tokens=3134, output_tokens=846
13:32:49,598 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:49,600 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 11.253000000026077. input_tokens=19, output_tokens=239
13:32:49,625 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:49,627 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 11.563000000081956. input_tokens=19, output_tokens=293
13:32:51,839 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:51,842 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 20.864999999990687. input_tokens=19, output_tokens=447
13:32:52,847 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:52,852 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 35.43900000001304. input_tokens=2764, output_tokens=650
13:32:53,446 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:53,450 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 11.737999999895692. input_tokens=19, output_tokens=224
13:32:54,444 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:54,447 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 22.485000000102445. input_tokens=19, output_tokens=472
13:32:54,559 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:54,562 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 26.972999999998137. input_tokens=19, output_tokens=573
13:32:54,684 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:54,686 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 9.971000000135973. input_tokens=19, output_tokens=226
13:32:54,757 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:54,759 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 13.900000000139698. input_tokens=19, output_tokens=313
13:32:54,931 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:54,936 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 16.597000000067055. input_tokens=19, output_tokens=300
13:32:55,570 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:55,574 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 15.652999999932945. input_tokens=19, output_tokens=395
13:32:55,758 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:55,763 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 21.010000000009313. input_tokens=19, output_tokens=452
13:32:55,785 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:55,787 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 19.040999999968335. input_tokens=19, output_tokens=408
13:32:56,27 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:56,31 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 34.82899999991059. input_tokens=3134, output_tokens=850
13:32:58,39 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:58,42 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 24.24600000004284. input_tokens=19, output_tokens=545
13:33:00,206 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:00,210 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 24.716999999945983. input_tokens=19, output_tokens=657
13:33:03,933 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:03,937 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 29.218000000109896. input_tokens=19, output_tokens=494
13:33:04,217 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:04,220 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 20.98300000000745. input_tokens=19, output_tokens=373
13:33:04,879 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:04,886 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 31.62000000011176. input_tokens=19, output_tokens=547
13:33:05,824 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:05,828 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 32.7589999998454. input_tokens=19, output_tokens=606
13:33:07,394 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:07,399 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 18.99500000011176. input_tokens=19, output_tokens=399
13:33:07,814 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:07,818 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 54.66299999994226. input_tokens=3134, output_tokens=1003
13:33:07,917 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:07,920 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 21.006000000052154. input_tokens=19, output_tokens=343
13:33:13,43 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:13,48 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 60.39699999988079. input_tokens=3133, output_tokens=1244
13:33:13,62 datashaper.workflow.workflow INFO executing verb merge_graphs
13:33:13,89 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
13:33:13,195 graphrag.index.run INFO Running workflow: create_summarized_entities...
13:33:13,195 graphrag.index.run INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
13:33:13,195 graphrag.index.run INFO read table from storage: create_base_extracted_entities.parquet
13:33:13,205 datashaper.workflow.workflow INFO executing verb summarize_descriptions
13:33:15,637 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:15,640 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.3959999999497086. input_tokens=184, output_tokens=50
13:33:16,64 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:16,66 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.8240000000223517. input_tokens=162, output_tokens=50
13:33:16,419 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:16,420 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.1640000001061708. input_tokens=192, output_tokens=62
13:33:17,522 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:17,525 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.262000000104308. input_tokens=179, output_tokens=64
13:33:17,541 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:17,542 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.277000000001863. input_tokens=162, output_tokens=61
13:33:17,611 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:17,613 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.371999999973923. input_tokens=173, output_tokens=65
13:33:17,745 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:17,748 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.509000000078231. input_tokens=205, output_tokens=102
13:33:18,48 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:18,51 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.782999999821186. input_tokens=186, output_tokens=99
13:33:18,145 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:18,147 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.877000000094995. input_tokens=187, output_tokens=116
13:33:18,623 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:18,626 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.365000000223517. input_tokens=185, output_tokens=96
13:33:18,683 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:18,685 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.42699999990873. input_tokens=172, output_tokens=98
13:33:19,186 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:19,190 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.915999999968335. input_tokens=182, output_tokens=135
13:33:20,352 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:20,355 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.111000000033528. input_tokens=246, output_tokens=135
13:33:20,552 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:20,554 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.8059999998658895. input_tokens=188, output_tokens=53
13:33:21,194 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:21,198 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.945000000065193. input_tokens=274, output_tokens=193
13:33:21,779 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:21,782 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.520000000018626. input_tokens=170, output_tokens=104
13:33:21,808 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:21,809 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.7579999999143183. input_tokens=178, output_tokens=75
13:33:21,839 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:21,840 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.574999999953434. input_tokens=196, output_tokens=105
13:33:21,958 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:21,960 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.703999999910593. input_tokens=223, output_tokens=194
13:33:22,68 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:22,70 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.817999999970198. input_tokens=286, output_tokens=190
13:33:22,449 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:22,451 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.908000000054017. input_tokens=199, output_tokens=101
13:33:22,644 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:22,646 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.390999999828637. input_tokens=308, output_tokens=244
13:33:22,693 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:22,695 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.628000000026077. input_tokens=300, output_tokens=181
13:33:22,742 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:22,743 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.5959999999031425. input_tokens=186, output_tokens=80
13:33:22,924 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:22,928 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.402000000001863. input_tokens=200, output_tokens=150
13:33:24,117 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:24,121 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.699999999953434. input_tokens=213, output_tokens=169
13:33:24,689 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:24,691 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.077000000048429. input_tokens=229, output_tokens=143
13:33:24,735 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:24,736 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.094000000040978. input_tokens=170, output_tokens=166
13:33:25,526 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:25,527 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.6869999999180436. input_tokens=200, output_tokens=78
13:33:25,528 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:25,531 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.84499999997206. input_tokens=173, output_tokens=70
13:33:25,631 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:25,632 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.1799999999348074. input_tokens=165, output_tokens=66
13:33:25,720 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:25,721 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.912000000011176. input_tokens=161, output_tokens=77
13:33:25,819 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:25,820 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 12.547999999951571. input_tokens=389, output_tokens=317
13:33:25,831 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:25,832 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.7619999998714775. input_tokens=177, output_tokens=75
13:33:26,342 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:26,343 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.415000000037253. input_tokens=154, output_tokens=76
13:33:26,355 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:26,356 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.6600000001490116. input_tokens=183, output_tokens=82
13:33:26,997 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:27,0 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.256000000052154. input_tokens=165, output_tokens=95
13:33:27,174 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:27,176 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.819999999832362. input_tokens=155, output_tokens=73
13:33:27,411 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:27,413 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.858000000007451. input_tokens=249, output_tokens=169
13:33:27,818 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:27,821 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.629999999888241. input_tokens=211, output_tokens=201
13:33:28,354 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:28,355 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.825999999884516. input_tokens=174, output_tokens=60
13:33:28,531 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:28,532 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 15.263999999966472. input_tokens=262, output_tokens=174
13:33:28,829 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:28,831 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.094000000040978. input_tokens=177, output_tokens=91
13:33:28,896 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:28,896 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:28,897 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 15.649999999906868. input_tokens=540, output_tokens=409
13:33:28,898 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.9359999999869615. input_tokens=183, output_tokens=123
13:33:29,69 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:29,72 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.4390000000130385. input_tokens=173, output_tokens=82
13:33:29,463 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:29,465 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 16.218000000109896. input_tokens=514, output_tokens=192
13:33:29,621 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:29,622 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.089999999850988. input_tokens=186, output_tokens=96
13:33:29,919 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:29,921 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.100000000093132. input_tokens=176, output_tokens=85
13:33:29,933 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:29,934 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.5200000000186265. input_tokens=150, output_tokens=51
13:33:30,39 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:30,40 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.841000000014901. input_tokens=235, output_tokens=186
13:33:30,80 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:30,81 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.7239999999292195. input_tokens=146, output_tokens=85
13:33:30,462 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:30,463 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.770999999949709. input_tokens=235, output_tokens=140
13:33:30,834 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:30,837 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.8359999998938292. input_tokens=139, output_tokens=61
13:33:31,213 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:31,214 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.3160000001080334. input_tokens=162, output_tokens=56
13:33:31,580 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:31,581 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.7579999999143183. input_tokens=186, output_tokens=72
13:33:31,769 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:31,770 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.938000000081956. input_tokens=173, output_tokens=87
13:33:31,801 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:31,802 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.080000000074506. input_tokens=193, output_tokens=99
13:33:31,818 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:31,819 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.464000000152737. input_tokens=169, output_tokens=60
13:33:32,105 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:32,107 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.033999999985099. input_tokens=166, output_tokens=47
13:33:32,117 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:32,118 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.334999999962747. input_tokens=375, output_tokens=269
13:33:32,607 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:32,609 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.5679999999701977. input_tokens=183, output_tokens=53
13:33:32,870 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:32,874 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 19.624000000068918. input_tokens=537, output_tokens=395
13:33:33,46 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:33,48 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.5839999997988343. input_tokens=155, output_tokens=44
13:33:33,252 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:33,255 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.173000000184402. input_tokens=185, output_tokens=72
13:33:33,333 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:33,336 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.688000000081956. input_tokens=341, output_tokens=220
13:33:33,605 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:33,607 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.141000000061467. input_tokens=171, output_tokens=82
13:33:33,885 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:33,887 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.98699999996461. input_tokens=166, output_tokens=47
13:33:33,957 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:33,958 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.1200000001117587. input_tokens=161, output_tokens=52
13:33:34,107 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:34,110 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.577000000048429. input_tokens=214, output_tokens=118
13:33:34,264 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:34,266 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.434000000124797. input_tokens=176, output_tokens=56
13:33:34,573 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:34,574 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.652999999932945. input_tokens=158, output_tokens=44
13:33:34,804 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:34,807 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.0360000000800937. input_tokens=162, output_tokens=61
13:33:34,825 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:34,826 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.0239999999757856. input_tokens=158, output_tokens=60
13:33:34,889 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:34,891 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.2679999999236315. input_tokens=169, output_tokens=55
13:33:34,967 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:34,970 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.034999999916181. input_tokens=163, output_tokens=51
13:33:35,150 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:35,153 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.975999999791384. input_tokens=324, output_tokens=169
13:33:36,374 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:36,376 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.257000000216067. input_tokens=170, output_tokens=61
13:33:36,496 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:36,498 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.916000000201166. input_tokens=183, output_tokens=73
13:33:36,541 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:36,542 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.493000000016764. input_tokens=162, output_tokens=45
13:33:36,761 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:36,763 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.8880000000353903. input_tokens=166, output_tokens=57
13:33:36,789 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:36,790 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.1820000000298023. input_tokens=187, output_tokens=67
13:33:36,814 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:36,815 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 23.57799999997951. input_tokens=534, output_tokens=356
13:33:37,122 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:37,125 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.165999999968335. input_tokens=167, output_tokens=64
13:33:37,170 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:37,172 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.0640000000130385. input_tokens=178, output_tokens=79
13:33:37,303 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:37,306 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.091000000014901. input_tokens=185, output_tokens=108
13:33:37,858 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:37,861 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.040999999968335. input_tokens=185, output_tokens=73
13:33:37,898 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:37,901 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.3260000001173466. input_tokens=183, output_tokens=62
13:33:38,15 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:38,17 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.406999999890104. input_tokens=164, output_tokens=67
13:33:38,96 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:38,98 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.1269999998621643. input_tokens=180, output_tokens=57
13:33:38,112 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:38,114 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.858000000007451. input_tokens=169, output_tokens=73
13:33:38,255 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:38,258 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.43100000009872. input_tokens=151, output_tokens=67
13:33:38,487 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:38,490 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.152999999932945. input_tokens=183, output_tokens=78
13:33:38,548 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:38,550 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.4390000000130385. input_tokens=162, output_tokens=74
13:33:38,931 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:38,935 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.125999999931082. input_tokens=159, output_tokens=55
13:33:38,947 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:38,949 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.682000000029802. input_tokens=163, output_tokens=73
13:33:39,42 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:39,45 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.8910000000614673. input_tokens=176, output_tokens=69
13:33:39,578 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:39,582 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.691000000108033. input_tokens=211, output_tokens=95
13:33:39,756 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:39,758 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.381999999983236. input_tokens=180, output_tokens=89
13:33:40,125 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:40,129 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.6299999998882413. input_tokens=168, output_tokens=76
13:33:40,295 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:40,297 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.408999999985099. input_tokens=191, output_tokens=90
13:33:40,670 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:40,675 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 22.04799999995157. input_tokens=604, output_tokens=488
13:33:40,948 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:40,951 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.408000000054017. input_tokens=180, output_tokens=107
13:33:47,339 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:47,342 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 23.219000000040978. input_tokens=531, output_tokens=390
13:33:49,486 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:49,494 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 23.148000000044703. input_tokens=659, output_tokens=500
13:33:49,525 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
13:33:49,621 graphrag.index.run INFO Running workflow: create_base_entity_graph...
13:33:49,622 graphrag.index.run INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
13:33:49,622 graphrag.index.run INFO read table from storage: create_summarized_entities.parquet
13:33:49,631 datashaper.workflow.workflow INFO executing verb cluster_graph
13:33:49,696 datashaper.workflow.workflow INFO executing verb select
13:33:49,698 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
13:33:49,838 graphrag.index.run INFO Running workflow: create_final_entities...
13:33:49,838 graphrag.index.run INFO dependencies for create_final_entities: ['create_base_entity_graph']
13:33:49,839 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
13:33:49,849 datashaper.workflow.workflow INFO executing verb unpack_graph
13:33:49,871 datashaper.workflow.workflow INFO executing verb rename
13:33:49,875 datashaper.workflow.workflow INFO executing verb select
13:33:49,880 datashaper.workflow.workflow INFO executing verb dedupe
13:33:49,885 datashaper.workflow.workflow INFO executing verb rename
13:33:49,890 datashaper.workflow.workflow INFO executing verb filter
13:33:49,902 datashaper.workflow.workflow INFO executing verb text_split
13:33:49,910 datashaper.workflow.workflow INFO executing verb drop
13:33:49,916 datashaper.workflow.workflow INFO executing verb merge
13:33:49,944 datashaper.workflow.workflow INFO executing verb text_embed
13:33:49,945 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
13:33:49,989 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
13:33:49,989 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
13:33:50,39 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 357 inputs via 357 snippets using 23 batches. max_batch_size=16, max_tokens=8191
13:33:50,311 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:33:50,321 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:33:50,324 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:33:50,352 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:33:50,356 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:33:50,369 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:33:50,375 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:33:50,378 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:33:50,389 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.325999999884516. input_tokens=106, output_tokens=0
13:33:50,396 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:33:50,419 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:33:50,420 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:33:50,435 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:33:50,442 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:33:50,449 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4039999998640269. input_tokens=675, output_tokens=0
13:33:50,469 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.40899999998509884. input_tokens=899, output_tokens=0
13:33:50,487 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4450000000651926. input_tokens=2064, output_tokens=0
13:33:50,499 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:33:50,501 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:33:50,501 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:33:50,503 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4520000000484288. input_tokens=1516, output_tokens=0
13:33:50,515 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.45600000000558794. input_tokens=976, output_tokens=0
13:33:50,528 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4739999999292195. input_tokens=1023, output_tokens=0
13:33:50,540 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:33:50,543 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4790000000502914. input_tokens=516, output_tokens=0
13:33:50,554 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5019999998621643. input_tokens=833, output_tokens=0
13:33:50,568 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:33:50,568 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:33:50,570 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5130000000353903. input_tokens=560, output_tokens=0
13:33:50,583 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:33:50,585 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5270000000018626. input_tokens=497, output_tokens=0
13:33:50,605 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5559999998658895. input_tokens=895, output_tokens=0
13:33:50,620 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5570000000298023. input_tokens=323, output_tokens=0
13:33:50,631 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:33:50,631 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:33:50,635 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5740000000223517. input_tokens=1085, output_tokens=0
13:33:50,649 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5879999999888241. input_tokens=681, output_tokens=0
13:33:50,663 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6160000001545995. input_tokens=728, output_tokens=0
13:33:50,680 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6340000000782311. input_tokens=652, output_tokens=0
13:33:50,692 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6469999998807907. input_tokens=1388, output_tokens=0
13:33:50,710 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6550000000279397. input_tokens=728, output_tokens=0
13:33:50,726 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6759999999776483. input_tokens=1185, output_tokens=0
13:33:50,761 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:33:50,772 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7160000000149012. input_tokens=614, output_tokens=0
13:33:50,791 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7360000000335276. input_tokens=608, output_tokens=0
13:33:50,804 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7519999998621643. input_tokens=713, output_tokens=0
13:33:50,829 datashaper.workflow.workflow INFO executing verb drop
13:33:50,835 datashaper.workflow.workflow INFO executing verb filter
13:33:50,850 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
13:33:51,9 graphrag.index.run INFO Running workflow: create_final_nodes...
13:33:51,10 graphrag.index.run INFO dependencies for create_final_nodes: ['create_base_entity_graph']
13:33:51,10 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
13:33:51,24 datashaper.workflow.workflow INFO executing verb layout_graph
13:33:51,103 datashaper.workflow.workflow INFO executing verb unpack_graph
13:33:51,132 datashaper.workflow.workflow INFO executing verb unpack_graph
13:33:51,159 datashaper.workflow.workflow INFO executing verb filter
13:33:51,176 datashaper.workflow.workflow INFO executing verb drop
13:33:51,182 datashaper.workflow.workflow INFO executing verb select
13:33:51,189 datashaper.workflow.workflow INFO executing verb rename
13:33:51,195 datashaper.workflow.workflow INFO executing verb convert
13:33:51,220 datashaper.workflow.workflow INFO executing verb join
13:33:51,232 datashaper.workflow.workflow INFO executing verb rename
13:33:51,234 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
13:33:51,381 graphrag.index.run INFO Running workflow: create_final_communities...
13:33:51,381 graphrag.index.run INFO dependencies for create_final_communities: ['create_base_entity_graph']
13:33:51,381 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
13:33:51,398 datashaper.workflow.workflow INFO executing verb unpack_graph
13:33:51,424 datashaper.workflow.workflow INFO executing verb unpack_graph
13:33:51,452 datashaper.workflow.workflow INFO executing verb aggregate_override
13:33:51,463 datashaper.workflow.workflow INFO executing verb join
13:33:51,474 datashaper.workflow.workflow INFO executing verb join
13:33:51,485 datashaper.workflow.workflow INFO executing verb concat
13:33:51,493 datashaper.workflow.workflow INFO executing verb filter
13:33:51,525 datashaper.workflow.workflow INFO executing verb aggregate_override
13:33:51,536 datashaper.workflow.workflow INFO executing verb join
13:33:51,547 datashaper.workflow.workflow INFO executing verb filter
13:33:51,567 datashaper.workflow.workflow INFO executing verb fill
13:33:51,575 datashaper.workflow.workflow INFO executing verb merge
13:33:51,585 datashaper.workflow.workflow INFO executing verb copy
13:33:51,616 datashaper.workflow.workflow INFO executing verb select
13:33:51,621 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
13:33:51,794 graphrag.index.run INFO Running workflow: join_text_units_to_entity_ids...
13:33:51,795 graphrag.index.run INFO dependencies for join_text_units_to_entity_ids: ['create_final_entities']
13:33:51,795 graphrag.index.run INFO read table from storage: create_final_entities.parquet
13:33:51,828 datashaper.workflow.workflow INFO executing verb select
13:33:51,837 datashaper.workflow.workflow INFO executing verb unroll
13:33:51,847 datashaper.workflow.workflow INFO executing verb aggregate_override
13:33:51,859 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_entity_ids.parquet
13:33:52,16 graphrag.index.run INFO Running workflow: create_final_relationships...
13:33:52,16 graphrag.index.run INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
13:33:52,17 graphrag.index.run INFO read table from storage: create_final_nodes.parquet
13:33:52,21 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
13:33:52,41 datashaper.workflow.workflow INFO executing verb unpack_graph
13:33:52,70 datashaper.workflow.workflow INFO executing verb filter
13:33:52,94 datashaper.workflow.workflow INFO executing verb rename
13:33:52,103 datashaper.workflow.workflow INFO executing verb filter
13:33:52,128 datashaper.workflow.workflow INFO executing verb drop
13:33:52,139 datashaper.workflow.workflow INFO executing verb compute_edge_combined_degree
13:33:52,151 datashaper.workflow.workflow INFO executing verb convert
13:33:52,172 datashaper.workflow.workflow INFO executing verb convert
13:33:52,174 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
13:33:52,328 graphrag.index.run INFO Running workflow: join_text_units_to_relationship_ids...
13:33:52,328 graphrag.index.run INFO dependencies for join_text_units_to_relationship_ids: ['create_final_relationships']
13:33:52,329 graphrag.index.run INFO read table from storage: create_final_relationships.parquet
13:33:52,353 datashaper.workflow.workflow INFO executing verb select
13:33:52,363 datashaper.workflow.workflow INFO executing verb unroll
13:33:52,375 datashaper.workflow.workflow INFO executing verb aggregate_override
13:33:52,388 datashaper.workflow.workflow INFO executing verb select
13:33:52,389 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_relationship_ids.parquet
13:33:52,566 graphrag.index.run INFO Running workflow: create_final_community_reports...
13:33:52,566 graphrag.index.run INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
13:33:52,567 graphrag.index.run INFO read table from storage: create_final_nodes.parquet
13:33:52,570 graphrag.index.run INFO read table from storage: create_final_relationships.parquet
13:33:52,594 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
13:33:52,610 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
13:33:52,623 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
13:33:52,636 datashaper.workflow.workflow INFO executing verb prepare_community_reports
13:33:52,637 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 357
13:33:52,666 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 357
13:33:52,720 datashaper.workflow.workflow INFO executing verb create_community_reports
13:34:16,729 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:34:16,731 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 24.002000000094995. input_tokens=2162, output_tokens=506
13:34:19,480 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:34:19,484 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 26.74600000004284. input_tokens=2233, output_tokens=618
13:34:19,724 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:34:19,726 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 26.976999999955297. input_tokens=2195, output_tokens=724
13:34:20,936 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:34:20,939 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 28.192000000039116. input_tokens=2150, output_tokens=619
13:34:21,518 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:34:21,523 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 28.770000000018626. input_tokens=5445, output_tokens=815
13:34:22,112 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:34:22,118 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 29.373000000137836. input_tokens=6053, output_tokens=828
13:34:23,493 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:34:23,498 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 30.762000000104308. input_tokens=5799, output_tokens=843
13:34:56,76 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:34:56,81 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 32.55300000007264. input_tokens=5761, output_tokens=824
13:34:57,191 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:34:57,195 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 33.65999999991618. input_tokens=5242, output_tokens=834
13:34:59,136 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:34:59,141 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 35.5769999998156. input_tokens=6229, output_tokens=803
13:35:00,807 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:35:00,810 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 37.25099999993108. input_tokens=5341, output_tokens=832
13:35:01,899 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:35:01,902 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 38.347000000067055. input_tokens=6058, output_tokens=910
13:35:04,40 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:35:04,42 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 40.493000000016764. input_tokens=6061, output_tokens=714
13:35:05,552 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:35:05,556 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 42.010999999940395. input_tokens=6480, output_tokens=1028
13:35:15,977 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:35:15,981 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 52.44100000010803. input_tokens=5847, output_tokens=921
13:35:16,18 datashaper.workflow.workflow INFO executing verb window
13:35:16,20 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
13:35:16,152 graphrag.index.run INFO Running workflow: create_final_text_units...
13:35:16,152 graphrag.index.run INFO dependencies for create_final_text_units: ['join_text_units_to_relationship_ids', 'join_text_units_to_entity_ids', 'create_base_text_units']
13:35:16,153 graphrag.index.run INFO read table from storage: join_text_units_to_relationship_ids.parquet
13:35:16,156 graphrag.index.run INFO read table from storage: join_text_units_to_entity_ids.parquet
13:35:16,158 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
13:35:16,184 datashaper.workflow.workflow INFO executing verb select
13:35:16,196 datashaper.workflow.workflow INFO executing verb rename
13:35:16,208 datashaper.workflow.workflow INFO executing verb join
13:35:16,236 datashaper.workflow.workflow INFO executing verb join
13:35:16,254 datashaper.workflow.workflow INFO executing verb aggregate_override
13:35:16,269 datashaper.workflow.workflow INFO executing verb select
13:35:16,270 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
13:35:16,454 graphrag.index.run INFO Running workflow: create_base_documents...
13:35:16,454 graphrag.index.run INFO dependencies for create_base_documents: ['create_final_text_units']
13:35:16,454 graphrag.index.run INFO read table from storage: create_final_text_units.parquet
13:35:16,483 datashaper.workflow.workflow INFO executing verb unroll
13:35:16,497 datashaper.workflow.workflow INFO executing verb select
13:35:16,511 datashaper.workflow.workflow INFO executing verb rename
13:35:16,524 datashaper.workflow.workflow INFO executing verb join
13:35:16,540 datashaper.workflow.workflow INFO executing verb aggregate_override
13:35:16,555 datashaper.workflow.workflow INFO executing verb join
13:35:16,574 datashaper.workflow.workflow INFO executing verb rename
13:35:16,587 datashaper.workflow.workflow INFO executing verb convert
13:35:16,625 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
13:35:16,791 graphrag.index.run INFO Running workflow: create_final_documents...
13:35:16,791 graphrag.index.run INFO dependencies for create_final_documents: ['create_base_documents']
13:35:16,792 graphrag.index.run INFO read table from storage: create_base_documents.parquet
13:35:16,822 datashaper.workflow.workflow INFO executing verb rename
13:35:16,824 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
