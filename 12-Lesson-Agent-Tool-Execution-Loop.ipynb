{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8: 12 - Anatomy of an AI Agent - Finishing OpenAI Function Calling Loop\n",
    "---------------------------------------------------------------------------------\n",
    "In this lesson, we will update our Agent base to handle the final steps of the OpenAI Function Calling process. When the AI Agent executes a tool, it should pack the results, the tool ID, and the tool name as a \"Tool\" message. This message needs to be sent back to the LLM to conclude the conversation by summarizing the results with the initial question or suggesting another tool if necessary.\n",
    "\n",
    "## Objectives\n",
    "* Update the Agent base to handle the final steps of tool execution.\n",
    "* Pack the tool execution results into a \"Tool\" message.\n",
    "* Send the \"Tool\" message back to the LLM for conversation conclusion.\n",
    "* Summarize the results or suggest another tool based on the conversation context.\n",
    "\n",
    "## What this session covers:\n",
    "* Defining the current agent structure, including the LLM Client, short-term memory, and tool execution.\n",
    "* Updating the Agent base to manage the final steps of tool execution.\n",
    "* Packing tool execution results into a structured \"Tool\" message.\n",
    "* Sending the `Tool` message back to the LLM.\n",
    "* Concluding the conversation with summarized results or suggesting another tool.\n",
    "* Integrating and testing the updated Agent base within the AI Agent framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Current Agent Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List\n",
    "import openai\n",
    "\n",
    "class OpenAIChatCompletion:\n",
    "    \"\"\"Interacts with OpenAI's API for chat completions.\"\"\"\n",
    "    def __init__(self, model: str, api_key: str = None, base_url: str = None):\n",
    "        self.client = openai.OpenAI(api_key=api_key, base_url=base_url)\n",
    "        self.model = model\n",
    "\n",
    "    def generate(self, messages: List[str], tools: List[Dict[str, Any]] = None, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Generates a response from OpenAI's API.\"\"\"\n",
    "        params = {'messages': messages, 'model': self.model, 'tools': tools, **kwargs}\n",
    "        response = self.client.chat.completions.create(**params)\n",
    "        return response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short-Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "class ChatMessageMemory:\n",
    "    \"\"\"Manages conversation context.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "    \n",
    "    def add_message(self, message: Dict):\n",
    "        \"\"\"Add a message to memory.\"\"\"\n",
    "        self.messages.append(message)\n",
    "    \n",
    "    def add_messages(self, messages: List[Dict]):\n",
    "        \"\"\"Add multiple messages to memory.\"\"\"\n",
    "        for message in messages:\n",
    "            self.add_message(message)\n",
    "    \n",
    "    def get_messages(self) -> List[Dict]:\n",
    "        \"\"\"Retrieve all messages.\"\"\"\n",
    "        return self.messages.copy()\n",
    "    \n",
    "    def reset_memory(self):\n",
    "        \"\"\"Clear all messages.\"\"\"\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, ValidationError\n",
    "from typing import Callable, Type\n",
    "from inspect import signature\n",
    "\n",
    "class AgentTool:\n",
    "    \"\"\"Encapsulates a Python function with Pydantic validation.\"\"\"\n",
    "    def __init__(self, func: Callable, args_model: Type[BaseModel]):\n",
    "        self.func = func\n",
    "        self.args_model = args_model\n",
    "        self.name = func.__name__\n",
    "        self.description = func.__doc__ or self.args_schema.get('description', '')\n",
    "\n",
    "    def to_openai_function_call_definition(self) -> dict:\n",
    "        \"\"\"Converts the tool to OpenAI Function Calling format.\"\"\"\n",
    "        schema_dict = self.args_schema\n",
    "        description = schema_dict.pop(\"description\", \"\")\n",
    "        return {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": self.name,\n",
    "                \"description\": description,\n",
    "                \"parameters\": schema_dict\n",
    "            }\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def args_schema(self) -> dict:\n",
    "        \"\"\"Returns the tool's function argument schema as a dictionary.\"\"\"\n",
    "        schema = self.args_model.model_json_schema()\n",
    "        schema.pop(\"title\", None)\n",
    "        return schema\n",
    "\n",
    "    def validate_json_args(self, json_string: str) -> bool:\n",
    "        \"\"\"Validate JSON string using the Pydantic model.\"\"\"\n",
    "        try:\n",
    "            validated_args = self.args_model.model_validate_json(json_string)\n",
    "            return isinstance(validated_args, self.args_model)\n",
    "        except ValidationError:\n",
    "            return False\n",
    "\n",
    "    def run(self, *args, **kwargs) -> Any:\n",
    "        \"\"\"Execute the function with validated arguments.\"\"\"\n",
    "        try:\n",
    "            # Handle positional arguments by converting them to keyword arguments\n",
    "            if args:\n",
    "                sig = signature(self.func)\n",
    "                arg_names = list(sig.parameters.keys())\n",
    "                kwargs.update(dict(zip(arg_names, args)))\n",
    "\n",
    "            # Validate arguments with the provided Pydantic schema\n",
    "            validated_args = self.args_model(**kwargs)\n",
    "            return self.func(**validated_args.model_dump())\n",
    "        except ValidationError as e:\n",
    "            raise ValueError(f\"Argument validation failed for tool '{self.name}': {str(e)}\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"An error occurred during the execution of tool '{self.name}': {str(e)}\")\n",
    "\n",
    "    def __call__(self, *args, **kwargs) -> Any:\n",
    "        \"\"\"Allow the AgentTool instance to be called like a regular function.\"\"\"\n",
    "        return self.run(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Tool Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional, Type\n",
    "from pydantic import BaseModel\n",
    "\n",
    "def check_docstring(func: Callable):\n",
    "    \"\"\"Ensure the function has a docstring.\"\"\"\n",
    "    if not func.__doc__:\n",
    "        raise ValueError(f\"Function '{func.__name__}' must have a docstring.\")\n",
    "\n",
    "def Tool(func: Optional[Callable] = None, *, args_model: Type[BaseModel]) -> AgentTool:\n",
    "    \"\"\"Decorator to wrap a function with an AgentTool instance.\"\"\"\n",
    "    def decorator(f: Callable) -> AgentTool:\n",
    "        check_docstring(f)\n",
    "        return AgentTool(f, args_model=args_model)\n",
    "    return decorator(func) if func else decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Tool Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional\n",
    "import json\n",
    "\n",
    "class AgentToolExecutor:\n",
    "    \"\"\"Manages tool registration and execution.\"\"\"\n",
    "    \n",
    "    def __init__(self, tools: Optional[List[AgentTool]] = None):\n",
    "        self.tools: Dict[str, AgentTool] = {}\n",
    "        if tools:\n",
    "            for tool in tools:\n",
    "                self.register_tool(tool)\n",
    "    \n",
    "    def register_tool(self, tool: AgentTool):\n",
    "        \"\"\"Registers a tool.\"\"\"\n",
    "        if tool.name in self.tools:\n",
    "            raise ValueError(f\"Tool '{tool.name}' is already registered.\")\n",
    "        self.tools[tool.name] = tool\n",
    "      \n",
    "    def execute(self, tool_name: str, tool_args: str) -> Any:\n",
    "        \"\"\"Executes a tool by name with given arguments.\"\"\"\n",
    "        print(f\"Checking if {tool_name} exists..\")\n",
    "        tool = self.tools.get(tool_name)\n",
    "        if not tool:\n",
    "            raise ValueError(f\"Tool '{tool_name}' not found.\")\n",
    "        try:\n",
    "            print(f\"Validating {tool_name} suggested args {tool_args}\")\n",
    "            if tool.validate_json_args(tool_args):\n",
    "                tool_args_dict = json.loads(tool_args)\n",
    "                print(f\"Executing {tool_name} with args: {tool_args}\")\n",
    "                return tool.run(**tool_args_dict)\n",
    "            else:\n",
    "                raise ValueError(f\"Error validating tool '{tool_name}' arguments.\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error executing tool '{tool_name}': {e}\") from e\n",
    "    \n",
    "    def get_tool_names(self) -> List[str]:\n",
    "        \"\"\"Returns a list of all registered tool names.\"\"\"\n",
    "        return list(self.tools.keys())\n",
    "    \n",
    "    def get_tool_details(self) -> str:\n",
    "        \"\"\"Returns details of all registered tools.\"\"\"\n",
    "        tools_info = [f\"{tool.name}: {tool.description} Args schema: {tool.args_schema['properties']}\" for tool in self.tools.values()]\n",
    "        return '\\n'.join(tools_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Integrates LLM client, tools, and memory.\"\"\"\n",
    "    def __init__(self, llm_client, system_message: Dict[str, str], tools=None):\n",
    "        self.llm_client = llm_client\n",
    "        self.tools = tools\n",
    "        self.memory = ChatMessageMemory()\n",
    "        self.system_message = system_message\n",
    "        # Adding a Tool Executor\n",
    "        self.executor = AgentToolExecutor()\n",
    "        # Adding a Tool History List\n",
    "        self.tool_history = []\n",
    "        \n",
    "        # Register and convert tools\n",
    "        if tools:\n",
    "            for tool in tools:\n",
    "                self.executor.register_tool(tool)\n",
    "            self.function_calls = [tool.to_openai_function_call_definition() for tool in tools]\n",
    "\n",
    "    def run(self, user_message: Dict[str, str]):\n",
    "        self.memory.add_message(user_message)\n",
    "        chat_history = [self.system_message] + self.memory.get_messages()\n",
    "        response = self.llm_client.generate(chat_history, tools=self.tools)\n",
    "        self.memory.add_message(response)\n",
    "        return response\n",
    "    \n",
    "    def run(self, user_message: Dict[str, str]):\n",
    "        \"\"\"Generates responses, manages tool calls, and updates memory.\"\"\"\n",
    "        self.memory.add_message(user_message)\n",
    "        chat_history = [self.system_message] + self.memory.get_messages() + self.tool_history\n",
    "        response = self.llm_client.generate(chat_history, tools=self.function_calls)\n",
    "        if response.tool_calls:\n",
    "            for tool in response.tool_calls:\n",
    "                tool_name = tool.function.name\n",
    "                tool_args = tool.function.arguments\n",
    "                try:\n",
    "                    execution_results = self.executor.execute(tool_name, tool_args)\n",
    "                    self.tool_history.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": tool.id,\n",
    "                        \"name\": tool_name,\n",
    "                        \"content\": str(execution_results)\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    raise ValueError(f\"Execution error in tool '{tool_name}': {e}\") from e\n",
    "            return self.tool_history\n",
    "        else:\n",
    "            self.memory.add_message(response)\n",
    "            self.tool_history = []\n",
    "            return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating Tool Execution Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Agent Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Integrates LLM client, tools, memory, and manages tool executions.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client, system_message: Dict[str, str], max_iterations: int = 10, tools: Optional[List[AgentTool]] = None):\n",
    "        self.llm_client = llm_client\n",
    "        self.executor = AgentToolExecutor()\n",
    "        self.memory = ChatMessageMemory()\n",
    "        self.system_message = system_message\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tool_history = []\n",
    "        self.function_calls = None\n",
    "        \n",
    "        # Register and convert tools\n",
    "        if tools:\n",
    "            for tool in tools:\n",
    "                self.executor.register_tool(tool)\n",
    "            self.function_calls = [tool.to_openai_function_call_definition() for tool in tools]\n",
    "\n",
    "    def run(self, user_message: Dict[str, str]):\n",
    "        \"\"\"Generates responses, manages tool calls, and updates memory.\"\"\"\n",
    "        self.memory.add_message(user_message)\n",
    "\n",
    "        for _ in range(self.max_iterations):\n",
    "            chat_history = [self.system_message] + self.memory.get_messages() + self.tool_history\n",
    "            print(f\"chat history sent: {chat_history}\")\n",
    "            response = self.llm_client.generate(chat_history, tools=self.function_calls)\n",
    "\n",
    "            if self.parse_response(response):\n",
    "                continue\n",
    "            else:\n",
    "                self.memory.add_message(response)\n",
    "                self.tool_history = []\n",
    "                return response\n",
    "\n",
    "    def parse_response(self, response) -> bool:\n",
    "        \"\"\"Executes tool calls suggested by the LLM and updates tool history.\"\"\"\n",
    "        if response.tool_calls:\n",
    "            self.tool_history.append(response)\n",
    "            for tool in response.tool_calls:\n",
    "                tool_name = tool.function.name\n",
    "                tool_args = tool.function.arguments\n",
    "                try:\n",
    "                    execution_results = self.executor.execute(tool_name, tool_args)\n",
    "                    self.tool_history.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": tool.id,\n",
    "                        \"name\": tool_name,\n",
    "                        \"content\": str(execution_results)\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    raise ValueError(f\"Execution error in tool '{tool_name}': {e}\") from e\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing New Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API from environment variable\n",
    "# import os\n",
    "# api_key = os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "api_key=\"\"\n",
    "\n",
    "client = OpenAIChatCompletion(\n",
    "    base_url='https://api.openai.com/v1',\n",
    "    model='gpt-4o',\n",
    "    api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define System messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system message\n",
    "system_message = {\"role\": \"system\", \"content\": \"You are a weather assistant.\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "import random\n",
    "\n",
    "class GetWeatherSchema(BaseModel):\n",
    "    \"\"\"Get weather information based on location.\"\"\"\n",
    "    location: str = Field(description=\"Location to get weather for\")\n",
    "\n",
    "@Tool(args_model=GetWeatherSchema)\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Gets weather information.\"\"\"\n",
    "    temperature = random.randint(60, 80)\n",
    "    return f\"{location}: {temperature}F.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class JumpSchema(BaseModel):\n",
    "    \"\"\"Jump a specific distance\"\"\"\n",
    "    distance: str = Field(description=\"Specific distance to jump for\")\n",
    "\n",
    "@Tool(args_model=JumpSchema)\n",
    "def jump(distance: str) -> str:\n",
    "    \"\"\"Jumps a specific distance.\"\"\"\n",
    "    return f\"I jumped the following distance {distance}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_weather,jump]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Agent with the LLM client, system message, and tools\n",
    "agent = Agent(llm_client=client, system_message=system_message, tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send a User Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat history sent: [{'role': 'system', 'content': 'You are a weather assistant.'}, {'role': 'user', 'content': 'What is the weather in Virginia?'}]\n",
      "Checking if get_weather exists..\n",
      "Validating get_weather suggested args {\"location\":\"Virginia\"}\n",
      "Executing get_weather with args: {\"location\":\"Virginia\"}\n",
      "chat history sent: [{'role': 'system', 'content': 'You are a weather assistant.'}, {'role': 'user', 'content': 'What is the weather in Virginia?'}, ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_kVvSM3bYimghvHvefVCvmHQI', function=Function(arguments='{\"location\":\"Virginia\"}', name='get_weather'), type='function')]), {'role': 'tool', 'tool_call_id': 'call_kVvSM3bYimghvHvefVCvmHQI', 'name': 'get_weather', 'content': 'Virginia: 77F.'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'content': 'The current temperature in Virginia is 77°F.',\n",
       " 'role': 'assistant'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a user message\n",
    "user_message = {\"role\": \"user\", \"content\": \"What is the weather in Virginia?\"}\n",
    "\n",
    "# Generate a response using the agent\n",
    "response = agent.run(user_message)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Tool Executions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Memory Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'What is the weather in Virginia?'},\n",
       " {'content': 'The current temperature in Virginia is 77°F.',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.memory.get_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.memory.reset_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Tool Executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat history sent: [{'role': 'system', 'content': 'You are a weather assistant.'}, {'role': 'user', 'content': 'What is the weather in Virginia, New york and Gdansk?'}]\n",
      "Checking if get_weather exists..\n",
      "Validating get_weather suggested args {\"location\": \"Virginia\"}\n",
      "Executing get_weather with args: {\"location\": \"Virginia\"}\n",
      "Checking if get_weather exists..\n",
      "Validating get_weather suggested args {\"location\": \"New York\"}\n",
      "Executing get_weather with args: {\"location\": \"New York\"}\n",
      "Checking if get_weather exists..\n",
      "Validating get_weather suggested args {\"location\": \"Gdansk\"}\n",
      "Executing get_weather with args: {\"location\": \"Gdansk\"}\n",
      "chat history sent: [{'role': 'system', 'content': 'You are a weather assistant.'}, {'role': 'user', 'content': 'What is the weather in Virginia, New york and Gdansk?'}, ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_gNi6QLLVsHY0wOy4QrSWIiD4', function=Function(arguments='{\"location\": \"Virginia\"}', name='get_weather'), type='function'), ChatCompletionMessageToolCall(id='call_TpxQ9hk24iG1IJhD4eT50SuV', function=Function(arguments='{\"location\": \"New York\"}', name='get_weather'), type='function'), ChatCompletionMessageToolCall(id='call_cnjA03cBtoOddJ8LYPnGXgTI', function=Function(arguments='{\"location\": \"Gdansk\"}', name='get_weather'), type='function')]), {'role': 'tool', 'tool_call_id': 'call_gNi6QLLVsHY0wOy4QrSWIiD4', 'name': 'get_weather', 'content': 'Virginia: 70F.'}, {'role': 'tool', 'tool_call_id': 'call_TpxQ9hk24iG1IJhD4eT50SuV', 'name': 'get_weather', 'content': 'New York: 69F.'}, {'role': 'tool', 'tool_call_id': 'call_cnjA03cBtoOddJ8LYPnGXgTI', 'name': 'get_weather', 'content': 'Gdansk: 64F.'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'content': 'Here is the current weather for the specified locations:\\n- Virginia: 70°F\\n- New York: 69°F\\n- Gdansk: 64°F',\n",
       " 'role': 'assistant'}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a user message\n",
    "user_message = {\"role\": \"user\", \"content\": \"What is the weather in Virginia, New york and Gdansk?\"}\n",
    "\n",
    "# Generate a response using the agent\n",
    "response = agent.run(user_message)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
