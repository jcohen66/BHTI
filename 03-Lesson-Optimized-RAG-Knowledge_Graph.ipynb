{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8: 3 - Optimizing RAG - Knowledge Graph\n",
    "----------------------------------------------------------------------------\n",
    "In this lesson, we will explore the latest RAG approach from the Microsoft research team, [GraphRAG](https://microsoft.github.io/graphrag/). It is a structured and hierarchical method for Retrieval Augmented Generation (RAG). Unlike simple semantic-search techniques that use plain text snippets, GraphRAG involves creating a knowledge graph from raw text, organizing this into a community hierarchy, generating summaries for these communities, and utilizing these structures for RAG-based tasks.\n",
    "\n",
    "## Objectives\n",
    "* Understand GraphRAG\n",
    "* Implement GraphRAG to refine the retrieval process.\n",
    "* Utilize knowledge graphs for enhanced retrieval accuracy.\n",
    "* Test and validate GraphRAG using enriched ATT&CK groups data.\n",
    "\n",
    "## What this session covers:\n",
    "* Indexing groups data using GraphRAG\n",
    "* Exploring the different component of GraphRAG\n",
    "* Executing global and local search prompts\n",
    "* Demonstrating and testing optimization scenarios with ATT&CK groups data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphrag\n",
      "  Downloading graphrag-0.3.6-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting aiofiles<25.0.0,>=24.1.0 (from graphrag)\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting aiolimiter<2.0.0,>=1.1.0 (from graphrag)\n",
      "  Downloading aiolimiter-1.1.0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting azure-identity<2.0.0,>=1.17.1 (from graphrag)\n",
      "  Downloading azure_identity-1.19.0-py3-none-any.whl.metadata (80 kB)\n",
      "Collecting azure-search-documents<12.0.0,>=11.4.0 (from graphrag)\n",
      "  Downloading azure_search_documents-11.5.2-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting azure-storage-blob<13.0.0,>=12.22.0 (from graphrag)\n",
      "  Downloading azure_storage_blob-12.23.1-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting datashaper<0.0.50,>=0.0.49 (from graphrag)\n",
      "  Downloading datashaper-0.0.49-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting devtools<0.13.0,>=0.12.2 (from graphrag)\n",
      "  Downloading devtools-0.12.2-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting environs<12.0.0,>=11.0.0 (from graphrag)\n",
      "  Downloading environs-11.0.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting future<2.0.0,>=1.0.0 (from graphrag)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting graspologic<4.0.0,>=3.4.1 (from graphrag)\n",
      "  Downloading graspologic-3.4.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting json-repair<0.29.0,>=0.28.4 (from graphrag)\n",
      "  Downloading json_repair-0.28.4-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting lancedb<0.14.0,>=0.13.0 (from graphrag)\n",
      "  Downloading lancedb-0.13.0-cp38-abi3-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting matplotlib<4.0.0,>=3.9.0 (from graphrag)\n",
      "  Downloading matplotlib-3.9.2-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: networkx<4,>=3 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from graphrag) (3.4.2)\n",
      "Collecting nltk==3.9.1 (from graphrag)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.25.2 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from graphrag) (1.26.4)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.46.1 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from graphrag) (1.53.0)\n",
      "Collecting pandas<3.0.0,>=2.2.2 (from graphrag)\n",
      "  Downloading pandas-2.2.3-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting pyaml-env<2.0.0,>=1.2.1 (from graphrag)\n",
      "  Downloading pyaml_env-1.2.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pyarrow<16.0.0,>=15.0.0 (from graphrag)\n",
      "  Downloading pyarrow-15.0.2-cp311-cp311-win_amd64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.2 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from graphrag) (2.9.2)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.0 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from graphrag) (1.0.1)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from graphrag) (6.0.2)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.6.0 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from graphrag) (13.9.1)\n",
      "Requirement already satisfied: tenacity<10.0.0,>=9.0.0 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from graphrag) (9.0.0)\n",
      "Collecting tiktoken<0.8.0,>=0.7.0 (from graphrag)\n",
      "  Downloading tiktoken-0.7.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.12.2 in c:\\users\\jcohe\\appdata\\roaming\\python\\python311\\site-packages (from graphrag) (4.12.2)\n",
      "Collecting umap-learn<0.6.0,>=0.5.6 (from graphrag)\n",
      "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting click (from nltk==3.9.1->graphrag)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk==3.9.1->graphrag) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk==3.9.1->graphrag) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk==3.9.1->graphrag) (4.66.6)\n",
      "Collecting azure-core>=1.31.0 (from azure-identity<2.0.0,>=1.17.1->graphrag)\n",
      "  Downloading azure_core-1.32.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting cryptography>=2.5 (from azure-identity<2.0.0,>=1.17.1->graphrag)\n",
      "  Downloading cryptography-43.0.3-cp39-abi3-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting msal>=1.30.0 (from azure-identity<2.0.0,>=1.17.1->graphrag)\n",
      "  Downloading msal-1.31.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting msal-extensions>=1.2.0 (from azure-identity<2.0.0,>=1.17.1->graphrag)\n",
      "  Downloading msal_extensions-1.2.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting azure-common>=1.1 (from azure-search-documents<12.0.0,>=11.4.0->graphrag)\n",
      "  Downloading azure_common-1.1.28-py2.py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting isodate>=0.6.0 (from azure-search-documents<12.0.0,>=11.4.0->graphrag)\n",
      "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting diskcache<6.0.0,>=5.6.3 (from datashaper<0.0.50,>=0.0.49->graphrag)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.21.1 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datashaper<0.0.50,>=0.0.49->graphrag) (4.23.0)\n",
      "Requirement already satisfied: asttokens<3.0.0,>=2.0.0 in c:\\users\\jcohe\\appdata\\roaming\\python\\python311\\site-packages (from devtools<0.13.0,>=0.12.2->graphrag) (2.4.1)\n",
      "Requirement already satisfied: executing>=1.1.1 in c:\\users\\jcohe\\appdata\\roaming\\python\\python311\\site-packages (from devtools<0.13.0,>=0.12.2->graphrag) (2.1.0)\n",
      "Requirement already satisfied: pygments>=2.15.0 in c:\\users\\jcohe\\appdata\\roaming\\python\\python311\\site-packages (from devtools<0.13.0,>=0.12.2->graphrag) (2.18.0)\n",
      "Requirement already satisfied: marshmallow>=3.13.0 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from environs<12.0.0,>=11.0.0->graphrag) (3.23.1)\n",
      "Collecting POT<0.10,>=0.9 (from graspologic<4.0.0,>=3.4.1->graphrag)\n",
      "  Downloading POT-0.9.4-cp311-cp311-win_amd64.whl.metadata (33 kB)\n",
      "Collecting anytree<3.0.0,>=2.12.1 (from graspologic<4.0.0,>=3.4.1->graphrag)\n",
      "  Downloading anytree-2.12.1-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting beartype<0.19.0,>=0.18.5 (from graspologic<4.0.0,>=3.4.1->graphrag)\n",
      "  Downloading beartype-0.18.5-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting gensim<5.0.0,>=4.3.2 (from graspologic<4.0.0,>=3.4.1->graphrag)\n",
      "  Downloading gensim-4.3.3-cp311-cp311-win_amd64.whl.metadata (8.2 kB)\n",
      "Collecting graspologic-native<2.0.0,>=1.2.1 (from graspologic<4.0.0,>=3.4.1->graphrag)\n",
      "  Downloading graspologic_native-1.2.1-cp36-abi3-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting hyppo<0.5.0,>=0.4.0 (from graspologic<4.0.0,>=3.4.1->graphrag)\n",
      "  Downloading hyppo-0.4.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: scikit-learn<2.0.0,>=1.4.2 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from graspologic<4.0.0,>=3.4.1->graphrag) (1.5.2)\n",
      "Collecting scipy==1.12.0 (from graspologic<4.0.0,>=3.4.1->graphrag)\n",
      "  Downloading scipy-1.12.0-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting seaborn<0.14.0,>=0.13.2 (from graspologic<4.0.0,>=3.4.1->graphrag)\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting statsmodels<0.15.0,>=0.14.2 (from graspologic<4.0.0,>=3.4.1->graphrag)\n",
      "  Downloading statsmodels-0.14.4-cp311-cp311-win_amd64.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: deprecation in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from lancedb<0.14.0,>=0.13.0->graphrag) (2.1.0)\n",
      "Collecting pylance==0.17.0 (from lancedb<0.14.0,>=0.13.0->graphrag)\n",
      "  Downloading pylance-0.17.0-cp39-abi3-win_amd64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from lancedb<0.14.0,>=0.13.0->graphrag) (2.32.3)\n",
      "Requirement already satisfied: retry>=0.9.2 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from lancedb<0.14.0,>=0.13.0->graphrag) (0.9.2)\n",
      "Requirement already satisfied: attrs>=21.3.0 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from lancedb<0.14.0,>=0.13.0->graphrag) (24.2.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\jcohe\\appdata\\roaming\\python\\python311\\site-packages (from lancedb<0.14.0,>=0.13.0->graphrag) (24.1)\n",
      "Requirement already satisfied: cachetools in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from lancedb<0.14.0,>=0.13.0->graphrag) (5.5.0)\n",
      "Requirement already satisfied: overrides>=0.7 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from lancedb<0.14.0,>=0.13.0->graphrag) (7.3.1)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib<4.0.0,>=3.9.0->graphrag)\n",
      "  Downloading contourpy-1.3.0-cp311-cp311-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib<4.0.0,>=3.9.0->graphrag)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib<4.0.0,>=3.9.0->graphrag)\n",
      "  Downloading fonttools-4.54.1-cp311-cp311-win_amd64.whl.metadata (167 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib<4.0.0,>=3.9.0->graphrag)\n",
      "  Downloading kiwisolver-1.4.7-cp311-cp311-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib<4.0.0,>=3.9.0->graphrag) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib<4.0.0,>=3.9.0->graphrag) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jcohe\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib<4.0.0,>=3.9.0->graphrag) (2.9.0.post0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai<2.0.0,>=1.46.1->graphrag) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai<2.0.0,>=1.46.1->graphrag) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai<2.0.0,>=1.46.1->graphrag) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai<2.0.0,>=1.46.1->graphrag) (0.7.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai<2.0.0,>=1.46.1->graphrag) (1.3.1)\n",
      "Collecting pytz>=2020.1 (from pandas<3.0.0,>=2.2.2->graphrag)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas<3.0.0,>=2.2.2->graphrag) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.9.2->graphrag) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.9.2->graphrag) (2.23.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich<14.0.0,>=13.6.0->graphrag) (3.0.0)\n",
      "Collecting numba>=0.51.2 (from umap-learn<0.6.0,>=0.5.6->graphrag)\n",
      "  Downloading numba-0.60.0-cp311-cp311-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting pynndescent>=0.5 (from umap-learn<0.6.0,>=0.5.6->graphrag)\n",
      "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.46.1->graphrag) (3.10)\n",
      "Requirement already satisfied: six in c:\\users\\jcohe\\appdata\\roaming\\python\\python311\\site-packages (from anytree<3.0.0,>=2.12.1->graspologic<4.0.0,>=3.4.1->graphrag) (1.16.0)\n",
      "Collecting cffi>=1.12 (from cryptography>=2.5->azure-identity<2.0.0,>=1.17.1->graphrag)\n",
      "  Downloading cffi-1.17.1-cp311-cp311-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim<5.0.0,>=4.3.2->graspologic<4.0.0,>=3.4.1->graphrag)\n",
      "  Downloading smart_open-7.0.5-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.46.1->graphrag) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.46.1->graphrag) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.46.1->graphrag) (0.14.0)\n",
      "Collecting autograd>=1.3 (from hyppo<0.5.0,>=0.4.0->graspologic<4.0.0,>=3.4.1->graphrag)\n",
      "  Downloading autograd-1.7.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema<5.0.0,>=4.21.1->datashaper<0.0.50,>=0.0.49->graphrag) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema<5.0.0,>=4.21.1->datashaper<0.0.50,>=0.0.49->graphrag) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema<5.0.0,>=4.21.1->datashaper<0.0.50,>=0.0.49->graphrag) (0.20.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.6.0->graphrag) (0.1.2)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity<2.0.0,>=1.17.1->graphrag) (2.9.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.4 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from msal-extensions>=1.2.0->azure-identity<2.0.0,>=1.17.1->graphrag) (2.10.1)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba>=0.51.2->umap-learn<0.6.0,>=0.5.6->graphrag)\n",
      "  Downloading llvmlite-0.43.0-cp311-cp311-win_amd64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.31.0->lancedb<0.14.0,>=0.13.0->graphrag) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.31.0->lancedb<0.14.0,>=0.13.0->graphrag) (2.2.3)\n",
      "Requirement already satisfied: decorator>=3.4.2 in c:\\users\\jcohe\\appdata\\roaming\\python\\python311\\site-packages (from retry>=0.9.2->lancedb<0.14.0,>=0.13.0->graphrag) (5.1.1)\n",
      "Requirement already satisfied: py<2.0.0,>=1.4.26 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from retry>=0.9.2->lancedb<0.14.0,>=0.13.0->graphrag) (1.11.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn<2.0.0,>=1.4.2->graspologic<4.0.0,>=3.4.1->graphrag) (3.5.0)\n",
      "Collecting patsy>=0.5.6 (from statsmodels<0.15.0,>=0.14.2->graspologic<4.0.0,>=3.4.1->graphrag)\n",
      "  Downloading patsy-0.5.6-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\jcohe\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->nltk==3.9.1->graphrag) (0.4.6)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=2.5->azure-identity<2.0.0,>=1.17.1->graphrag)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\jcohe\\appdata\\roaming\\python\\python311\\site-packages (from portalocker<3,>=1.4->msal-extensions>=1.2.0->azure-identity<2.0.0,>=1.17.1->graphrag) (308)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.2->graspologic<4.0.0,>=3.4.1->graphrag)\n",
      "  Downloading wrapt-1.16.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Downloading graphrag-0.3.6-py3-none-any.whl (389 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 26.5 MB/s eta 0:00:00\n",
      "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading aiolimiter-1.1.0-py3-none-any.whl (7.2 kB)\n",
      "Downloading azure_identity-1.19.0-py3-none-any.whl (187 kB)\n",
      "Downloading azure_search_documents-11.5.2-py3-none-any.whl (298 kB)\n",
      "Downloading azure_storage_blob-12.23.1-py3-none-any.whl (405 kB)\n",
      "Downloading datashaper-0.0.49-py3-none-any.whl (71 kB)\n",
      "Downloading devtools-0.12.2-py3-none-any.whl (19 kB)\n",
      "Downloading environs-11.0.0-py3-none-any.whl (12 kB)\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Downloading graspologic-3.4.1-py3-none-any.whl (5.2 MB)\n",
      "   ---------------------------------------- 0.0/5.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.2/5.2 MB 53.1 MB/s eta 0:00:00\n",
      "Downloading scipy-1.12.0-cp311-cp311-win_amd64.whl (46.2 MB)\n",
      "   ---------------------------------------- 0.0/46.2 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 12.8/46.2 MB 57.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 22.5/46.2 MB 52.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 27.8/46.2 MB 44.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 34.3/46.2 MB 39.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 41.4/46.2 MB 38.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  46.1/46.2 MB 38.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 46.2/46.2 MB 34.6 MB/s eta 0:00:00\n",
      "Downloading json_repair-0.28.4-py3-none-any.whl (13 kB)\n",
      "Downloading lancedb-0.13.0-cp38-abi3-win_amd64.whl (23.7 MB)\n",
      "   ---------------------------------------- 0.0/23.7 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 6.6/23.7 MB 31.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 12.8/23.7 MB 31.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 17.8/23.7 MB 27.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.2/23.7 MB 25.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 23.7/23.7 MB 22.7 MB/s eta 0:00:00\n",
      "Downloading pylance-0.17.0-cp39-abi3-win_amd64.whl (27.0 MB)\n",
      "   ---------------------------------------- 0.0/27.0 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 4.2/27.0 MB 21.0 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 8.4/27.0 MB 20.0 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 12.3/27.0 MB 19.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 17.0/27.0 MB 20.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 22.0/27.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  27.0/27.0 MB 22.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 27.0/27.0 MB 20.9 MB/s eta 0:00:00\n",
      "Downloading matplotlib-3.9.2-cp311-cp311-win_amd64.whl (7.8 MB)\n",
      "   ---------------------------------------- 0.0/7.8 MB ? eta -:--:--\n",
      "   ----------------------------- ---------- 5.8/7.8 MB 29.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.8/7.8 MB 26.9 MB/s eta 0:00:00\n",
      "Downloading pandas-2.2.3-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 6.3/11.6 MB 32.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 29.1 MB/s eta 0:00:00\n",
      "Downloading pyaml_env-1.2.1-py3-none-any.whl (9.0 kB)\n",
      "Downloading pyarrow-15.0.2-cp311-cp311-win_amd64.whl (24.8 MB)\n",
      "   ---------------------------------------- 0.0/24.8 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 5.2/24.8 MB 26.5 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 10.7/24.8 MB 26.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 17.3/24.8 MB 27.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.8 MB 29.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.8/24.8 MB 26.2 MB/s eta 0:00:00\n",
      "Downloading tiktoken-0.7.0-cp311-cp311-win_amd64.whl (799 kB)\n",
      "   ---------------------------------------- 0.0/799.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 799.0/799.0 kB 16.8 MB/s eta 0:00:00\n",
      "Downloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
      "Downloading anytree-2.12.1-py3-none-any.whl (44 kB)\n",
      "Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
      "Downloading azure_core-1.32.0-py3-none-any.whl (198 kB)\n",
      "Downloading beartype-0.18.5-py3-none-any.whl (917 kB)\n",
      "   ---------------------------------------- 0.0/917.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 917.8/917.8 kB 21.1 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.0-cp311-cp311-win_amd64.whl (217 kB)\n",
      "Downloading cryptography-43.0.3-cp39-abi3-win_amd64.whl (3.1 MB)\n",
      "   ---------------------------------------- 0.0/3.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.1/3.1 MB 22.4 MB/s eta 0:00:00\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Downloading fonttools-4.54.1-cp311-cp311-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 25.0 MB/s eta 0:00:00\n",
      "Downloading gensim-4.3.3-cp311-cp311-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 5.5/24.0 MB 27.9 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 11.3/24.0 MB 28.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 16.0/24.0 MB 26.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.2/24.0 MB 26.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 24.5 MB/s eta 0:00:00\n",
      "Downloading graspologic_native-1.2.1-cp36-abi3-win_amd64.whl (188 kB)\n",
      "Downloading hyppo-0.4.0-py3-none-any.whl (146 kB)\n",
      "Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
      "Downloading kiwisolver-1.4.7-cp311-cp311-win_amd64.whl (56 kB)\n",
      "Downloading msal-1.31.0-py3-none-any.whl (113 kB)\n",
      "Downloading msal_extensions-1.2.0-py3-none-any.whl (19 kB)\n",
      "Downloading numba-0.60.0-cp311-cp311-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.7/2.7 MB 22.0 MB/s eta 0:00:00\n",
      "Downloading POT-0.9.4-cp311-cp311-win_amd64.whl (309 kB)\n",
      "Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading statsmodels-0.14.4-cp311-cp311-win_amd64.whl (9.9 MB)\n",
      "   ---------------------------------------- 0.0/9.9 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 4.7/9.9 MB 22.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.7/9.9 MB 23.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.9/9.9 MB 21.9 MB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading autograd-1.7.0-py3-none-any.whl (52 kB)\n",
      "Downloading cffi-1.17.1-cp311-cp311-win_amd64.whl (181 kB)\n",
      "Downloading llvmlite-0.43.0-cp311-cp311-win_amd64.whl (28.1 MB)\n",
      "   ---------------------------------------- 0.0/28.1 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 5.5/28.1 MB 27.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 11.3/28.1 MB 28.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 17.8/28.1 MB 28.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 25.4/28.1 MB 30.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 28.1/28.1 MB 28.3 MB/s eta 0:00:00\n",
      "Downloading patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
      "Downloading smart_open-7.0.5-py3-none-any.whl (61 kB)\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Downloading wrapt-1.16.0-cp311-cp311-win_amd64.whl (37 kB)\n",
      "Installing collected packages: pytz, azure-common, wrapt, scipy, pycparser, pyarrow, pyaml-env, patsy, llvmlite, kiwisolver, json-repair, isodate, graspologic-native, future, fonttools, diskcache, cycler, contourpy, click, beartype, autograd, anytree, aiolimiter, aiofiles, tiktoken, smart-open, pylance, POT, pandas, numba, nltk, matplotlib, environs, devtools, cffi, azure-core, statsmodels, seaborn, pynndescent, lancedb, hyppo, gensim, cryptography, azure-search-documents, umap-learn, datashaper, azure-storage-blob, msal, graspologic, msal-extensions, azure-identity, graphrag\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.14.1\n",
      "    Uninstalling scipy-1.14.1:\n",
      "      Successfully uninstalled scipy-1.14.1\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 17.0.0\n",
      "    Uninstalling pyarrow-17.0.0:\n",
      "      Successfully uninstalled pyarrow-17.0.0\n",
      "  Attempting uninstall: json-repair\n",
      "    Found existing installation: json_repair 0.30.0\n",
      "    Uninstalling json_repair-0.30.0:\n",
      "      Successfully uninstalled json_repair-0.30.0\n",
      "  Attempting uninstall: tiktoken\n",
      "    Found existing installation: tiktoken 0.8.0\n",
      "    Uninstalling tiktoken-0.8.0:\n",
      "      Successfully uninstalled tiktoken-0.8.0\n",
      "  Attempting uninstall: pylance\n",
      "    Found existing installation: pylance 0.18.2\n",
      "    Uninstalling pylance-0.18.2:\n",
      "      Successfully uninstalled pylance-0.18.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.0.0\n",
      "    Uninstalling pandas-2.0.0:\n",
      "      Successfully uninstalled pandas-2.0.0\n",
      "  Attempting uninstall: lancedb\n",
      "    Found existing installation: lancedb 0.14.0\n",
      "    Uninstalling lancedb-0.14.0:\n",
      "      Successfully uninstalled lancedb-0.14.0\n",
      "Successfully installed POT-0.9.4 aiofiles-24.1.0 aiolimiter-1.1.0 anytree-2.12.1 autograd-1.7.0 azure-common-1.1.28 azure-core-1.32.0 azure-identity-1.19.0 azure-search-documents-11.5.2 azure-storage-blob-12.23.1 beartype-0.18.5 cffi-1.17.1 click-8.1.7 contourpy-1.3.0 cryptography-43.0.3 cycler-0.12.1 datashaper-0.0.49 devtools-0.12.2 diskcache-5.6.3 environs-11.0.0 fonttools-4.54.1 future-1.0.0 gensim-4.3.3 graphrag-0.3.6 graspologic-3.4.1 graspologic-native-1.2.1 hyppo-0.4.0 isodate-0.7.2 json-repair-0.28.4 kiwisolver-1.4.7 lancedb-0.13.0 llvmlite-0.43.0 matplotlib-3.9.2 msal-1.31.0 msal-extensions-1.2.0 nltk-3.9.1 numba-0.60.0 pandas-2.2.3 patsy-0.5.6 pyaml-env-1.2.1 pyarrow-15.0.2 pycparser-2.22 pylance-0.17.0 pynndescent-0.5.13 pytz-2024.2 scipy-1.12.0 seaborn-0.13.2 smart-open-7.0.5 statsmodels-0.14.4 tiktoken-0.7.0 umap-learn-0.5.7 wrapt-1.16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\~-ipy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\~-ipy'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\~-ktoken'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "crewai-tools 0.13.2 requires beautifulsoup4>=4.12.3, which is not installed.\n",
      "embedchain 0.1.123 requires beautifulsoup4<5.0.0,>=4.12.2, which is not installed.\n",
      "embedchain 0.1.123 requires pypdf<6.0.0,>=5.0.0, which is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jcohe\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jcohe\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement os (from versions: none)\n",
      "ERROR: No matching distribution found for os\n",
      "ERROR: Could not find a version that satisfies the requirement subprocess (from versions: none)\n",
      "ERROR: No matching distribution found for subprocess\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (15.0.2)\n",
      "Requirement already satisfied: numpy<2,>=1.16.6 in c:\\users\\jcohe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyarrow) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install graphrag\n",
    "!pip install pandas\n",
    "!pip install os\n",
    "!pip install subprocess\n",
    "!pip install requests\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory where you want to create the .env file\n",
    "current_directory = os.path.dirname(\"__file__\")\n",
    "data_directory = os.path.join(current_directory, \"data\")\n",
    "env_file_path = os.path.join(data_directory, \".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating .env File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Ensure the data directory exists\n",
    "if not os.path.exists(data_directory):\n",
    "    print(\"[+] Creating data directory..\")\n",
    "    os.makedirs(data_directory)\n",
    "\n",
    "# Create the .env file and add the line to it\n",
    "with open(env_file_path, \"w\") as file:\n",
    "    file.write(\"GRAPHRAG_API_KEY=<api key>\")  # api key as 'string'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up GraphRAG Workspace Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠋ GraphRAG Indexer \n",
      "Initializing project at ./data\n",
      "⠋ GraphRAG Indexer \n",
      "Traceback (most recent call last):\n",
      "⠋ GraphRAG Indexer \n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "⠋ GraphRAG Indexer \n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "⠋ GraphRAG Indexer \n",
      "  File \n",
      "\"c:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\graph\n",
      "rag\\index\\__main__.py\", line 104, in <module>\n",
      "⠋ GraphRAG Indexer \n",
      "    index_cli(\n",
      "⠋ GraphRAG Indexer \n",
      "  File \n",
      "\"c:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\graph\n",
      "rag\\index\\cli.py\", line 126, in index_cli\n",
      "⠋ GraphRAG Indexer \n",
      "    _initialize_project_at(root_dir, progress_reporter)\n",
      "⠋ GraphRAG Indexer \n",
      "  File \n",
      "\"c:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\graph\n",
      "rag\\index\\cli.py\", line 199, in _initialize_project_at\n",
      "⠋ GraphRAG Indexer \n",
      "    raise ValueError(msg)\n",
      "⠋ GraphRAG Indexer \n",
      "ValueError: Project already initialized at data\n",
      "⠋ GraphRAG Indexer \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\paramiko\\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "c:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\paramiko\\transport.py:256: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n"
     ]
    }
   ],
   "source": [
    "# This line of code creates settings.yaml and .env files, and also prompts and cache directories\n",
    "\n",
    "!python -m graphrag.index --init --root ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update .env and settings.yaml files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update .env:\n",
    "\n",
    "```\n",
    "GRAPHRAG_API_KEY=<API_KEY>\n",
    "```\n",
    "\n",
    "Update settings.yaml:\n",
    "\n",
    "```\n",
    "input:\n",
    "  type: file # or blob\n",
    "  file_type: text # or csv\n",
    "  base_dir: \"input\"  ----> \"documents\"\n",
    "  file_encoding: utf-8\n",
    "  file_pattern: \".*\\\\.txt$\"  ----> \".*\\\\.md$\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Documents with GraphRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents were already indexed\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Direcotry 'output' is created after indexing documents\n",
    "if not os.path.exists(os.path.join(data_directory, \"output\")):\n",
    "    print(\"[+] Indexing documents..\")\n",
    "    subprocess.run(\n",
    "        [\"python3\", \"-m\", \"graphrag.index\", \"--root\", \"./data\"]\n",
    "    )  # takes around 4 minutes to complete (9 .md files)\n",
    "else:\n",
    "    print(\"Documents were already indexed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "pd.set_option(\"max_colwidth\", 100)\n",
    "\n",
    "# Defining artifacts directory\n",
    "artifacts_directory = os.path.join(data_directory, \"output/20240801-133147/artifacts\")\n",
    "\n",
    "communities = pd.read_parquet(\n",
    "    path=os.path.join(artifacts_directory, \"create_final_communities.parquet\")\n",
    ")\n",
    "community_reports = pd.read_parquet(\n",
    "    path=os.path.join(artifacts_directory, \"create_final_community_reports.parquet\")\n",
    ")\n",
    "entities = pd.read_parquet(\n",
    "    path=os.path.join(artifacts_directory, \"create_final_entities.parquet\")\n",
    ")\n",
    "nodes = pd.read_parquet(\n",
    "    path=os.path.join(artifacts_directory, \"create_final_nodes.parquet\")\n",
    ")\n",
    "relationships = pd.read_parquet(\n",
    "    path=os.path.join(artifacts_directory, \"create_final_relationships.parquet\")\n",
    ")\n",
    "text_units = pd.read_parquet(\n",
    "    path=os.path.join(artifacts_directory, \"create_final_text_units.parquet\")\n",
    ")\n",
    "documents = pd.read_parquet(\n",
    "    path=os.path.join(artifacts_directory, \"create_final_documents.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_unit_ids</th>\n",
       "      <th>raw_content</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afd6025b421d49a061e3a6acf74daedd</td>\n",
       "      <td>[8112029b261923f118b8ba9196d067c8, 9cba25ded5f4750aeafc8a92bcc4f160, ec2e57ce7860a4c68a1210df495...</td>\n",
       "      <td># FIN7 - G0046\\n\\n**Created**: 2017-05-31T21:32:09.460Z\\n\\n**Modified**: 2024-04-17T22:09:41.004...</td>\n",
       "      <td>FIN7.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2362aa068c9293d37a7f821c903e629c</td>\n",
       "      <td>[bdbda95d8a567d3c9ee1d96ffbd1e2c0, 5e839788284eecb63814ac126969a91f, 51659aba82df667a55ca598ef7d...</td>\n",
       "      <td># Sandworm Team - G0034\\n\\n**Created**: 2017-05-31T21:32:04.588Z\\n\\n**Modified**: 2024-04-06T19:...</td>\n",
       "      <td>Sandworm_Team.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  afd6025b421d49a061e3a6acf74daedd   \n",
       "1  2362aa068c9293d37a7f821c903e629c   \n",
       "\n",
       "                                                                                         text_unit_ids  \\\n",
       "0  [8112029b261923f118b8ba9196d067c8, 9cba25ded5f4750aeafc8a92bcc4f160, ec2e57ce7860a4c68a1210df495...   \n",
       "1  [bdbda95d8a567d3c9ee1d96ffbd1e2c0, 5e839788284eecb63814ac126969a91f, 51659aba82df667a55ca598ef7d...   \n",
       "\n",
       "                                                                                           raw_content  \\\n",
       "0  # FIN7 - G0046\\n\\n**Created**: 2017-05-31T21:32:09.460Z\\n\\n**Modified**: 2024-04-17T22:09:41.004...   \n",
       "1  # Sandworm Team - G0034\\n\\n**Created**: 2017-05-31T21:32:04.588Z\\n\\n**Modified**: 2024-04-06T19:...   \n",
       "\n",
       "              title  \n",
       "0           FIN7.md  \n",
       "1  Sandworm_Team.md  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>document_ids</th>\n",
       "      <th>entity_ids</th>\n",
       "      <th>relationship_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bdbda95d8a567d3c9ee1d96ffbd1e2c0</td>\n",
       "      <td># Sandworm Team - G0034\\n\\n**Created**: 2017-05-31T21:32:04.588Z\\n\\n**Modified**: 2024-04-06T19:...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[2362aa068c9293d37a7f821c903e629c]</td>\n",
       "      <td>[b45241d70f0e43fca764df95b2b81f77, 4119fd06010c494caa07f439b333f4c5, d3835bf3dda84ead99deadbeac5...</td>\n",
       "      <td>[36be44627ece444284f9e759b8cd25c6, a64b4b17b07a44e4b1ac33580d811936, 423b72bbd56f4caa98f3328202c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5e839788284eecb63814ac126969a91f</td>\n",
       "      <td>HTML tags for the communication traffic between the C2 server.(Citation: ESET Telebots Dec 2016...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[2362aa068c9293d37a7f821c903e629c]</td>\n",
       "      <td>[b45241d70f0e43fca764df95b2b81f77, 96aad7cb4b7d40e9b7e13b94a67af206, c9632a35146940c2a86167c7726...</td>\n",
       "      <td>[bdddcb17ba6c408599dd395ce64f960a, bc70fee2061541148833d19e86f225b3, 0fc15cc3b44c4142a770feb4c03...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  bdbda95d8a567d3c9ee1d96ffbd1e2c0   \n",
       "1  5e839788284eecb63814ac126969a91f   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0  # Sandworm Team - G0034\\n\\n**Created**: 2017-05-31T21:32:04.588Z\\n\\n**Modified**: 2024-04-06T19:...   \n",
       "1   HTML tags for the communication traffic between the C2 server.(Citation: ESET Telebots Dec 2016...   \n",
       "\n",
       "   n_tokens                        document_ids  \\\n",
       "0      1200  [2362aa068c9293d37a7f821c903e629c]   \n",
       "1      1200  [2362aa068c9293d37a7f821c903e629c]   \n",
       "\n",
       "                                                                                            entity_ids  \\\n",
       "0  [b45241d70f0e43fca764df95b2b81f77, 4119fd06010c494caa07f439b333f4c5, d3835bf3dda84ead99deadbeac5...   \n",
       "1  [b45241d70f0e43fca764df95b2b81f77, 96aad7cb4b7d40e9b7e13b94a67af206, c9632a35146940c2a86167c7726...   \n",
       "\n",
       "                                                                                      relationship_ids  \n",
       "0  [36be44627ece444284f9e759b8cd25c6, a64b4b17b07a44e4b1ac33580d811936, 423b72bbd56f4caa98f3328202c...  \n",
       "1  [bdddcb17ba6c408599dd395ce64f960a, bc70fee2061541148833d19e86f225b3, 0fc15cc3b44c4142a770feb4c03...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_units.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>graph_embedding</th>\n",
       "      <th>text_unit_ids</th>\n",
       "      <th>description_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4119fd06010c494caa07f439b333f4c5</td>\n",
       "      <td>\"RUSSIA'S GENERAL STAFF MAIN INTELLIGENCE DIRECTORATE (GRU) MAIN CENTER FOR SPECIAL TECHNOLOGIES...</td>\n",
       "      <td>\"ORGANIZATION\"</td>\n",
       "      <td>\"Russia's GRU GTsST, military unit 74455, is the organization behind Sandworm Team, involved in ...</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>[bdbda95d8a567d3c9ee1d96ffbd1e2c0]</td>\n",
       "      <td>[-0.03776426985859871, 0.02152342163026333, 0.05562674254179001, 0.02057747170329094, -0.0243981...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>e683130322ac47708a852a5e51abb7c5</td>\n",
       "      <td>\"COBALT GROUP\"</td>\n",
       "      <td>\"ORGANIZATION\"</td>\n",
       "      <td>\"Cobalt Group is a cybercriminal group that may be linked to Carbanak and has used Carbanak malw...</td>\n",
       "      <td>294</td>\n",
       "      <td>None</td>\n",
       "      <td>[b45050169dbd3a4df2093131b2c6a8ef]</td>\n",
       "      <td>[-0.007689749822020531, -0.027905846014618874, 0.04049476236104965, 0.059866175055503845, 0.0257...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   id  \\\n",
       "1    4119fd06010c494caa07f439b333f4c5   \n",
       "115  e683130322ac47708a852a5e51abb7c5   \n",
       "\n",
       "                                                                                                    name  \\\n",
       "1    \"RUSSIA'S GENERAL STAFF MAIN INTELLIGENCE DIRECTORATE (GRU) MAIN CENTER FOR SPECIAL TECHNOLOGIES...   \n",
       "115                                                                                       \"COBALT GROUP\"   \n",
       "\n",
       "               type  \\\n",
       "1    \"ORGANIZATION\"   \n",
       "115  \"ORGANIZATION\"   \n",
       "\n",
       "                                                                                             description  \\\n",
       "1    \"Russia's GRU GTsST, military unit 74455, is the organization behind Sandworm Team, involved in ...   \n",
       "115  \"Cobalt Group is a cybercriminal group that may be linked to Carbanak and has used Carbanak malw...   \n",
       "\n",
       "     human_readable_id graph_embedding                       text_unit_ids  \\\n",
       "1                    1            None  [bdbda95d8a567d3c9ee1d96ffbd1e2c0]   \n",
       "115                294            None  [b45050169dbd3a4df2093131b2c6a8ef]   \n",
       "\n",
       "                                                                                   description_embedding  \n",
       "1    [-0.03776426985859871, 0.02152342163026333, 0.05562674254179001, 0.02057747170329094, -0.0243981...  \n",
       "115  [-0.007689749822020531, -0.027905846014618874, 0.04049476236104965, 0.059866175055503845, 0.0257...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities.sample(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\"ORGANIZATION\"', '\"GEO\"', '\"EVENT\"', '', '\"ACTIVITY\"', '\"TARGET\"',\n",
       "       '\"CONCEPT\"', '\"OBJECT\"', '\"DOCUMENT\"', '\"TECHNOLOGY\"',\n",
       "       '\"INDUSTRY\"', '\"INFORMATION\"', '\"TECHNIQUE\"', '\"FILE\"', '\"TACTIC\"',\n",
       "       '\"METHOD\"', '\"PERSON\"', '\"SOFTWARE\"', '\"LOCATION\"', '\"SECTOR\"',\n",
       "       '\"API\"', '\"TOOL\"', '\"MALWARE\"', '\"ENTITY\"', '\"CATEGORY\"',\n",
       "       '\"ORGANIZATION TYPE\"'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities[\"type\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>weight</th>\n",
       "      <th>description</th>\n",
       "      <th>text_unit_ids</th>\n",
       "      <th>id</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>source_degree</th>\n",
       "      <th>target_degree</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"SANDWORM TEAM\"</td>\n",
       "      <td>\"RUSSIA'S GENERAL STAFF MAIN INTELLIGENCE DIRECTORATE (GRU) MAIN CENTER FOR SPECIAL TECHNOLOGIES...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Sandworm Team is attributed to and operates under the GRU GTsST military unit 74455.\"</td>\n",
       "      <td>[bdbda95d8a567d3c9ee1d96ffbd1e2c0]</td>\n",
       "      <td>36be44627ece444284f9e759b8cd25c6</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"SANDWORM TEAM\"</td>\n",
       "      <td>\"GRU UNIT 74455\"</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Sandworm Team, directly associated with GRU Unit 74455, conducts cyber operations under its aegi...</td>\n",
       "      <td>[bdbda95d8a567d3c9ee1d96ffbd1e2c0, e8027f448bcfaa176bfb6c261f96b671]</td>\n",
       "      <td>a64b4b17b07a44e4b1ac33580d811936</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            source  \\\n",
       "0  \"SANDWORM TEAM\"   \n",
       "1  \"SANDWORM TEAM\"   \n",
       "\n",
       "                                                                                                target  \\\n",
       "0  \"RUSSIA'S GENERAL STAFF MAIN INTELLIGENCE DIRECTORATE (GRU) MAIN CENTER FOR SPECIAL TECHNOLOGIES...   \n",
       "1                                                                                     \"GRU UNIT 74455\"   \n",
       "\n",
       "   weight  \\\n",
       "0     1.0   \n",
       "1     2.0   \n",
       "\n",
       "                                                                                           description  \\\n",
       "0               \"Sandworm Team is attributed to and operates under the GRU GTsST military unit 74455.\"   \n",
       "1  Sandworm Team, directly associated with GRU Unit 74455, conducts cyber operations under its aegi...   \n",
       "\n",
       "                                                          text_unit_ids  \\\n",
       "0                                    [bdbda95d8a567d3c9ee1d96ffbd1e2c0]   \n",
       "1  [bdbda95d8a567d3c9ee1d96ffbd1e2c0, e8027f448bcfaa176bfb6c261f96b671]   \n",
       "\n",
       "                                 id human_readable_id  source_degree  \\\n",
       "0  36be44627ece444284f9e759b8cd25c6                 0             42   \n",
       "1  a64b4b17b07a44e4b1ac33580d811936                 1             42   \n",
       "\n",
       "   target_degree  rank  \n",
       "0              1    43  \n",
       "1              1    43  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relationships.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>level</th>\n",
       "      <th>raw_community</th>\n",
       "      <th>relationship_ids</th>\n",
       "      <th>text_unit_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Community 0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[36be44627ece444284f9e759b8cd25c6, a64b4b17b07a44e4b1ac33580d811936, 423b72bbd56f4caa98f3328202c...</td>\n",
       "      <td>[2dc4bd2cb621b71f05523705af6b571f,396c22047983755b945f5235333819d3,51659aba82df667a55ca598ef7d76...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Community 4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[30a251bc3d04430d82b5a1a98c7b8c75, 93e1d19f9bfa4c6b8962d56d10ea9483, 8046335ba70b434aa3188392a74...</td>\n",
       "      <td>[202bcc7099b468087583d88e9ddb57de,396c22047983755b945f5235333819d3,4e30c37f12c9daf946a2010e949a6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id        title  level raw_community  \\\n",
       "0  0  Community 0      0             0   \n",
       "1  4  Community 4      0             4   \n",
       "\n",
       "                                                                                      relationship_ids  \\\n",
       "0  [36be44627ece444284f9e759b8cd25c6, a64b4b17b07a44e4b1ac33580d811936, 423b72bbd56f4caa98f3328202c...   \n",
       "1  [30a251bc3d04430d82b5a1a98c7b8c75, 93e1d19f9bfa4c6b8962d56d10ea9483, 8046335ba70b434aa3188392a74...   \n",
       "\n",
       "                                                                                         text_unit_ids  \n",
       "0  [2dc4bd2cb621b71f05523705af6b571f,396c22047983755b945f5235333819d3,51659aba82df667a55ca598ef7d76...  \n",
       "1  [202bcc7099b468087583d88e9ddb57de,396c22047983755b945f5235333819d3,4e30c37f12c9daf946a2010e949a6...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "communities.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>community</th>\n",
       "      <th>full_content</th>\n",
       "      <th>level</th>\n",
       "      <th>rank</th>\n",
       "      <th>title</th>\n",
       "      <th>rank_explanation</th>\n",
       "      <th>summary</th>\n",
       "      <th>findings</th>\n",
       "      <th>full_content_json</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td># Carbanak Group and Carbanak Malware\\n\\nThe community focuses on the Carbanak Group, known for ...</td>\n",
       "      <td>1</td>\n",
       "      <td>8.5</td>\n",
       "      <td>Carbanak Group and Carbanak Malware</td>\n",
       "      <td>The high impact severity rating reflects the significant threat posed by the Carbanak Group's ac...</td>\n",
       "      <td>The community focuses on the Carbanak Group, known for its use of Carbanak malware, highlighting...</td>\n",
       "      <td>[{'explanation': 'The Carbanak Group is a prominent entity within the cybercrime community, know...</td>\n",
       "      <td>{\\n    \"title\": \"Carbanak Group and Carbanak Malware\",\\n    \"summary\": \"The community focuses on...</td>\n",
       "      <td>1bd35e5c-af30-4e33-b898-a8d58ef37083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td># Turla Cyber Espionage Activities and Infrastructure Targets\\n\\nThis report delves into the sop...</td>\n",
       "      <td>1</td>\n",
       "      <td>8.5</td>\n",
       "      <td>Turla Cyber Espionage Activities and Infrastructure Targets</td>\n",
       "      <td>The high impact severity rating reflects Turla's sophisticated capabilities and strategic target...</td>\n",
       "      <td>This report delves into the sophisticated cyber espionage activities of the Turla group, focusin...</td>\n",
       "      <td>[{'explanation': 'Turla, a group with ties to Russia's FSB, has been active since at least 2004,...</td>\n",
       "      <td>{\\n    \"title\": \"Turla Cyber Espionage Activities and Infrastructure Targets\",\\n    \"summary\": \"...</td>\n",
       "      <td>4b8eb4db-bf82-4576-920a-ebf15d72fb95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  community  \\\n",
       "0        10   \n",
       "1        11   \n",
       "\n",
       "                                                                                          full_content  \\\n",
       "0  # Carbanak Group and Carbanak Malware\\n\\nThe community focuses on the Carbanak Group, known for ...   \n",
       "1  # Turla Cyber Espionage Activities and Infrastructure Targets\\n\\nThis report delves into the sop...   \n",
       "\n",
       "   level  rank                                                        title  \\\n",
       "0      1   8.5                          Carbanak Group and Carbanak Malware   \n",
       "1      1   8.5  Turla Cyber Espionage Activities and Infrastructure Targets   \n",
       "\n",
       "                                                                                      rank_explanation  \\\n",
       "0  The high impact severity rating reflects the significant threat posed by the Carbanak Group's ac...   \n",
       "1  The high impact severity rating reflects Turla's sophisticated capabilities and strategic target...   \n",
       "\n",
       "                                                                                               summary  \\\n",
       "0  The community focuses on the Carbanak Group, known for its use of Carbanak malware, highlighting...   \n",
       "1  This report delves into the sophisticated cyber espionage activities of the Turla group, focusin...   \n",
       "\n",
       "                                                                                              findings  \\\n",
       "0  [{'explanation': 'The Carbanak Group is a prominent entity within the cybercrime community, know...   \n",
       "1  [{'explanation': 'Turla, a group with ties to Russia's FSB, has been active since at least 2004,...   \n",
       "\n",
       "                                                                                     full_content_json  \\\n",
       "0  {\\n    \"title\": \"Carbanak Group and Carbanak Malware\",\\n    \"summary\": \"The community focuses on...   \n",
       "1  {\\n    \"title\": \"Turla Cyber Espionage Activities and Infrastructure Targets\",\\n    \"summary\": \"...   \n",
       "\n",
       "                                     id  \n",
       "0  1bd35e5c-af30-4e33-b898-a8d58ef37083  \n",
       "1  4b8eb4db-bf82-4576-920a-ebf15d72fb95  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community_reports.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "id: string\n",
       "name: string\n",
       "type: string\n",
       "description: string\n",
       "human_readable_id: int64\n",
       "graph_embedding: null\n",
       "text_unit_ids: list<element: string>\n",
       "  child 0, element: string\n",
       "description_embedding: list<element: double>\n",
       "  child 0, element: double\n",
       "__index_level_0__: int64\n",
       "----\n",
       "id: [[\"b45241d70f0e43fca764df95b2b81f77\",\"4119fd06010c494caa07f439b333f4c5\",\"d3835bf3dda84ead99deadbeac5d0d7d\",\"077d2820ae1845bcbb1803379a3d1eae\",\"3671ea0dd4e84c1a9b02c5ab2c8f4bac\",...,\"67f10971666240ea930f3b875aabdc1a\",\"8b95083939ad4771b57a97c2d5805f36\",\"3c4062de44d64870a3cc5913d5769244\",\"24652fab20d84381b112b8491de2887e\",\"d4602d4a27b34358baa86814a3836d68\"]]\n",
       "name: [[\"\"SANDWORM TEAM\"\",\"\"RUSSIA'S GENERAL STAFF MAIN INTELLIGENCE DIRECTORATE (GRU) MAIN CENTER FOR SPECIAL TECHNOLOGIES (GTSST)\"\",\"\"GRU UNIT 74455\"\",\"\"GRU UNIT 26165\"\",\"\"UKRAINE\"\",...,\"\"REMOTE SYSTEM DISCOVERY\"\",\"\"SOFTWARE PACKING\"\",\"\"LOCAL ACCOUNT\"\",\"\"SYSTEM OWNER/USER DISCOVERY\"\",\"\"INDICATOR REMOVAL FROM TOOLS\"\"]]\n",
       "type: [[\"\"ORGANIZATION\"\",\"\"ORGANIZATION\"\",\"\"ORGANIZATION\"\",\"\"ORGANIZATION\"\",\"\"GEO\"\",...,\"\",\"\",\"\",\"\",\"\"EVENT\"\"]]\n",
       "description: [[\"Sandworm Team, also known by multiple aliases including ELECTRUM, Telebots, IRON VIKING, BlackEnergy Group, Quedagh, Voodoo Bear, IRIDIUM, Seashell Blizzard, and FROZENBARENTS, is a highly sophisticated and destructive cyber threat group. Attributed to Russia's GRU GTsST military unit 74455, Sandworm Team has been active since at least 2009. This group is involved in a wide array of cyber operations aimed at compromising systems, stealing information, and causing disruption. Their activities include the development and deployment of malware, the use of VPN tunnels, botnets, phishing campaigns, and acquiring legitimate credentials to launch attacks. They are also known for data destruction, running an SSH server, pushing malicious tools, collecting usernames, obtaining email addresses for spearphishing, and more.\n",
       "\n",
       "Sandworm Team targets enterprise databases, systems, and networks through data exfiltration, exploitation of vulnerabilities, and the use of remote access software. They compromise servers and conduct research against potential victims to tailor their attacks. Their techniques are varied and sophisticated, encompassing the use of webshells, backdoors, denial of service attacks, software supply chain compromises, keylogging, disk structure wipes, and data encryption to maximize impact. Additionally, they engage in software research for supply-chain operations, external defacement of websites, and infrastructure acquisition for phishing campaigns.\n",
       "\n",
       "The group is also known for leasing servers, conducting technical reconnaissance, deploying malware, exploiting vulnerabilities, and executing commands through different software and methods to achieve their objectives. Sandworm Team's broad spectrum of malicious activities underscores its capability and intent to conduct significant cyber operations against a variety of targets, making it a formidable force in the cyber threat landscape.\",\"\"Russia's GRU GTsST, military unit 74455, is the organization behind Sandworm Team, involved in cyber espionage and destructive cyber operations.\"\",\"GRU Unit 74455, associated with Sandworm Team, has been mentioned in indictments for its involvement in a series of high-profile cyber operations. These operations include attacks against Ukrainian electrical companies, the deployment of the NotPetya malware, targeting of the French presidential campaign, the Olympic Destroyer attack, and operations against the Organisation for the Prohibition of Chemical Weapons (OPCW) and the country of Georgia. This unit's activities highlight its significant role in global cyber espionage and cyber warfare efforts.\",\"\"GRU Unit 26165, also referred to as APT28, collaborated with Sandworm Team in conducting cyber operations.\"\",\"Ukraine has been a significant target of cyber attacks by groups such as Sandworm Team, particularly in 2015 and 2016. These attacks have affected electrical companies and government organizations, highlighting the country's vulnerability to cyber threats and the focused efforts of attackers like the Sandworm Team to disrupt its infrastructure and governmental functions.\",...,\"\",\"\",\"\",\"System Owner/User Discovery is a cyber espionage technique employed by APT3, aimed at verifying elevated system privileges. This method is part of their sophisticated toolkit to infiltrate and maneuver within compromised networks, ensuring they have the necessary access levels for their malicious activities.\",\"\"Indicator Removal from Tools is a technique used by APT3 to remove indicators of compromise from their tools.\"\"]]\n",
       "human_readable_id: [[0,1,2,3,4,...,352,353,354,355,356]]\n",
       "graph_embedding: [357 nulls]\n",
       "text_unit_ids: [[[\"2dc4bd2cb621b71f05523705af6b571f\",\"396c22047983755b945f5235333819d3\",\"51659aba82df667a55ca598ef7d761f4\",\"57e7a82d79f46331d434ee95a5702d63\",\"5e839788284eecb63814ac126969a91f\",\"a1a3fe0ef9fd26359bc5b35d3e744e0b\",\"bdbda95d8a567d3c9ee1d96ffbd1e2c0\",\"e8027f448bcfaa176bfb6c261f96b671\"],[\"bdbda95d8a567d3c9ee1d96ffbd1e2c0\"],...,[\"202bcc7099b468087583d88e9ddb57de\",\"59e034e04f5a7416ebfcb5d607182fd2\"],[\"202bcc7099b468087583d88e9ddb57de\"]]]\n",
       "description_embedding: [[[0.022811800241470337,0.04036780819296837,0.04281684383749962,0.04548601433634758,-0.005489691626280546,...,0.0010215779766440392,0.011116969399154186,0.030213940888643265,-0.025797422975301743,0.018120110034942627],[-0.03776426985859871,0.02152342163026333,0.05562674254179001,0.02057747170329094,-0.02439812570810318,...,-0.021879687905311584,0.01128382608294487,0.01786247454583645,-0.024299845099449158,0.04995104670524597],...,[-0.004239504225552082,0.00457511842250824,0.013323252089321613,0.01080297864973545,0.0014081549597904086,...,-0.03649963065981865,-0.011721168644726276,-0.009618830867111683,-0.025164734572172165,0.004780919756740332],[0.010909955948591232,0.04998791962862015,0.0263220127671957,0.001015960588119924,-0.018592743203043938,...,0.00834681373089552,-0.008008159697055817,-0.0015189606929197907,-0.012576662935316563,-0.0017563501605764031]]]\n",
       "__index_level_0__: [[0,1,2,3,4,...,173,174,175,176,177]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another way using pyarrow\n",
    "import pyarrow.parquet as pa\n",
    "\n",
    "table = pa.read_table(\n",
    "    source=os.path.join(artifacts_directory, \"create_final_entities.parquet\")\n",
    ")\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Search: Understanding the Knowledge Corpus\n",
    "\"For reasoning about holistic questions about the corpus by leveraging the community summaries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "creating llm client with {'api_key': 'REDACTED,len=9', 'type': \"openai_chat\", 'model': 'gpt-4-turbo-preview', 'max_tokens': 4000, 'temperature': 0.0, 'top_p': 1.0, 'n': 1, 'request_timeout': 180.0, 'api_base': None, 'api_version': None, 'organization': None, 'proxy': None, 'cognitive_services_endpoint': None, 'deployment_name': None, 'model_supports_json': True, 'tokens_per_minute': 0, 'requests_per_minute': 0, 'max_retries': 10, 'max_retry_wait': 10.0, 'sleep_on_rate_limit_recommendation': True, 'concurrent_requests': 25}\n",
      "\n",
      "SUCCESS: Global Search Response:\n",
      "I am sorry but I am unable to answer this question given the provided data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\paramiko\\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "c:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\paramiko\\transport.py:256: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n",
      "Exception in _map_response_single_batch\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\graphrag\\query\\structured_search\\global_search\\search.py\", line 214, in _map_response_single_batch\n",
      "    search_response = await self.llm.agenerate(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\graphrag\\query\\llm\\oai\\chat_openai.py\", line 142, in agenerate\n",
      "    async for attempt in retryer:\n",
      "  File \"c:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n",
      "    do = await self.iter(retry_state=self._retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\graphrag\\query\\llm\\oai\\chat_openai.py\", line 144, in agenerate\n",
      "    return await self._agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\graphrag\\query\\llm\\oai\\chat_openai.py\", line 268, in _agenerate\n",
      "    response = await self.async_client.chat.completions.create(  # type: ignore\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1633, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py\", line 1838, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py\", line 1532, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py\", line 1633, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: <api key>. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "Warning: All map responses have score 0 (i.e., no relevant information found from the dataset), returning a canned 'I do not know' answer. You can try enabling `allow_general_knowledge` to encourage the LLM to incorporate relevant general knowledge, at the risk of increasing hallucinations.\n"
     ]
    }
   ],
   "source": [
    "!python -m graphrag.query --root ./data --method global \"What are the top 3 most common techniques used by adversary groups?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Search: Exploring Specific Entities and their Context\n",
    "\n",
    "\"for reasoning about specific entities by fanning-out to their neighbors and associated concepts.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "INFO: Reading settings from data/settings.yaml\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-08-05T15:05:30Z \u001b[0m\u001b[33mWARN \u001b[0m lance::dataset\u001b[0m\u001b[38;5;8m]\u001b[0m No existing dataset at /Users/panda/Documents/CyberChasquis/BTA_4Days/module-8/lancedb/description_embedding.lance, it will be created\n",
      "creating llm client with {'api_key': 'REDACTED,len=132', 'type': \"openai_chat\", 'model': 'gpt-4-turbo-preview', 'max_tokens': 4000, 'temperature': 0.0, 'top_p': 1.0, 'n': 1, 'request_timeout': 180.0, 'api_base': None, 'api_version': None, 'organization': None, 'proxy': None, 'cognitive_services_endpoint': None, 'deployment_name': None, 'model_supports_json': True, 'tokens_per_minute': 0, 'requests_per_minute': 0, 'max_retries': 10, 'max_retry_wait': 10.0, 'sleep_on_rate_limit_recommendation': True, 'concurrent_requests': 25}\n",
      "creating embedding llm client with {'api_key': 'REDACTED,len=132', 'type': \"openai_embedding\", 'model': 'text-embedding-3-small', 'max_tokens': 4000, 'temperature': 0, 'top_p': 1, 'n': 1, 'request_timeout': 180.0, 'api_base': None, 'api_version': None, 'organization': None, 'proxy': None, 'cognitive_services_endpoint': None, 'deployment_name': None, 'model_supports_json': None, 'tokens_per_minute': 0, 'requests_per_minute': 0, 'max_retries': 10, 'max_retry_wait': 10.0, 'sleep_on_rate_limit_recommendation': True, 'concurrent_requests': 25}\n",
      "\n",
      "SUCCESS: Local Search Response: The data provided outlines a variety of sophisticated techniques employed by adversary groups, particularly focusing on APT3 and APT29, which are attributed to China's Ministry of State Security and Russia's Foreign Intelligence Service (SVR), respectively. These groups have demonstrated a wide range of cyber espionage activities targeting Linux, macOS, and Windows operating systems. By analyzing the techniques mentioned across the data, we can identify some of the most common tactics used by these groups.\n",
      "\n",
      "### 1. Spearphishing\n",
      "\n",
      "Spearphishing remains a prevalent technique used by both APT3 and APT29, leveraging social engineering to compromise security systems. APT3 has been known to send spearphishing emails containing malicious links as part of its social engineering tactics to breach security systems [Data: Sources (42)]. Similarly, APT29 has engaged in spearphishing campaigns, including sending emails with malicious links to gain access to victim accounts [Data: Entities (300); Relationships (109, 115, +more)]. This method exploits human vulnerabilities, demonstrating the critical importance of vigilance in digital communications.\n",
      "\n",
      "### 2. Exploitation of Vulnerabilities\n",
      "\n",
      "Both groups have extensively utilized vulnerability exploitation as a core component of their operations. APT3 has exploited vulnerabilities in Adobe Flash Player and Internet Explorer to execute client-side attacks [Data: Sources (44)]. APT29 has also been noted for its exploitation of vulnerabilities for privilege escalation and to gain unauthorized access [Data: Entities (300); Relationships (74, +more)]. This technique underscores the groups' advanced capabilities in identifying and leveraging weaknesses in software or systems to manipulate or damage targeted systems or software.\n",
      "\n",
      "### 3. Obfuscation Techniques\n",
      "\n",
      "APT3, in particular, has demonstrated the use of obfuscation techniques to evade defensive measures. This includes obfuscating files or information to help evade detection [Data: Entities (340); Sources (42)]. While specific obfuscation techniques used by APT29 are not detailed in the provided data, the group's sophisticated nature and diverse cyber attack techniques suggest a likely utilization of similar tactics to evade detection and maintain persistence within compromised networks.\n",
      "\n",
      "In summary, spearphishing, exploitation of vulnerabilities, and obfuscation techniques are among the top tactics employed by these adversary groups. These methods highlight the multifaceted nature of cyber threats, combining technical exploits with psychological manipulation to achieve their objectives. The reliance on such techniques underscores the continuous arms race between attackers and defenders in the cybersecurity domain.\n"
     ]
    }
   ],
   "source": [
    "!python3 -m graphrag.query --root ./data --method local \"What are the top 3 most common techniques used by adversary groups?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Local Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "INFO: Vector Store Args: {}\n",
      "creating llm client with {'api_key': 'REDACTED,len=164', 'type': \"openai_chat\", 'model': 'gpt-4-turbo-preview', 'max_tokens': 4000, 'temperature': 0.0, 'top_p': 1.0, 'n': 1, 'request_timeout': 180.0, 'api_base': None, 'api_version': None, 'organization': None, 'proxy': None, 'cognitive_services_endpoint': None, 'deployment_name': None, 'model_supports_json': True, 'tokens_per_minute': 0, 'requests_per_minute': 0, 'max_retries': 10, 'max_retry_wait': 10.0, 'sleep_on_rate_limit_recommendation': True, 'concurrent_requests': 25}\n",
      "creating embedding llm client with {'api_key': 'REDACTED,len=164', 'type': \"openai_embedding\", 'model': 'text-embedding-3-small', 'max_tokens': 4000, 'temperature': 0, 'top_p': 1, 'n': 1, 'request_timeout': 180.0, 'api_base': None, 'api_version': None, 'organization': None, 'proxy': None, 'cognitive_services_endpoint': None, 'deployment_name': None, 'model_supports_json': None, 'tokens_per_minute': 0, 'requests_per_minute': 0, 'max_retries': 10, 'max_retry_wait': 10.0, 'sleep_on_rate_limit_recommendation': True, 'concurrent_requests': 25}\n",
      "\n",
      "SUCCESS: Local Search Response:\n",
      "## Overview of Wizard Spider\n",
      "\n",
      "Wizard Spider is a Russia-based cybercriminal group known for its sophisticated and diverse cyber-attack techniques. Active since at least 2016, this group has gained notoriety for the creation and deployment of TrickBot, a malware used extensively in their operations. Wizard Spider's activities encompass a wide range of cybercrimes, including ransomware campaigns using Ryuk, credential theft, data exfiltration, and disabling security measures across various organizations, such as major corporations and hospitals [Data: Entities (267)].\n",
      "\n",
      "## Main Relationships and Activities\n",
      "\n",
      "### Geographical Origin\n",
      "Wizard Spider is based in Russia, indicating the geographical origin of its operations. This connection underscores the group's potential access to resources and networks within Russia [Data: Relationships (207)].\n",
      "\n",
      "### Targeted Operating Systems\n",
      "Wizard Spider specifically targets multiple operating systems in their cyber-attack campaigns, employing a range of techniques. They have been documented targeting Linux, Windows, and macOS, showcasing their ability to adapt and execute attacks across different platforms. Their operations focus on exploiting vulnerabilities within these operating systems to achieve their malicious objectives [Data: Relationships (46, 53, 60)].\n",
      "\n",
      "### Malware Deployment\n",
      "A significant aspect of Wizard Spider's operations involves the creation and deployment of malware, notably TrickBot and Ryuk ransomware. TrickBot is primarily delivered through spearphishing attachments and serves multiple malicious purposes, including financial fraud and acting as a dropper for other malware. Ryuk ransomware is used by Wizard Spider to execute payloads that are hosted on network file shares among other malicious purposes [Data: Relationships (208, 210)].\n",
      "\n",
      "### Cybersecurity Firm Engagements\n",
      "Wizard Spider has been the subject of reports and analyses by prominent cybersecurity firms, including Mandiant and CrowdStrike. These firms have documented the group's tactics and techniques, contributing to the broader cybersecurity community's understanding of Wizard Spider's operations. Mandiant, in particular, has reported on the activities and tactics of Wizard Spider, including their use of PowerShell and other techniques [Data: Relationships (217, 111)].\n",
      "\n",
      "### Collaboration and Documentation\n",
      "Individuals such as Oleksiy Gayda and Edward Millington have contributed to the analysis or documentation of Wizard Spider's activities, indicating a collaborative effort within the cybersecurity research community to understand and counteract the group's operations [Data: Relationships (206, 145)].\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Wizard Spider represents a significant threat to global cybersecurity, with a proven track record of evolving tactics and techniques to evade detection and maximize the impact of their attacks. Their ability to target a wide range of operating systems and employ sophisticated malware like TrickBot and Ryuk ransomware underscores the advanced nature of their operations. The involvement of cybersecurity firms in documenting and analyzing Wizard Spider's activities highlights the importance of collaborative efforts in mitigating the threats posed by this group.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\paramiko\\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "c:\\Users\\jcohe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\paramiko\\transport.py:256: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n",
      "[2024-11-03T22:19:57Z WARN  lance::dataset] No existing dataset at C:\\Users\\jcohe\\Code\\VisualStudioProjects\\BTA_4Days-main\\module-8\\data\\output\\20240801-133147\\artifacts\\lancedb\\entity_description_embeddings.lance, it will be created\n"
     ]
    }
   ],
   "source": [
    "!python -m graphrag.query --root ./data --method local \"What do we know about adversary group Wizard Spider? What are its main relationship?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Welcome to GRAPHRAG: https://microsoft.github.io/graphrag/\n",
    "\n",
    "- Properties of intrusion set here: https://github.com/oasis-open/cti-python-stix2/blob/50fd81fd6ba4f26824a864319305bc298e89bb45/stix2/v21/sdo.py#L314\n",
    "\n",
    "- Reading Parquet files using Pandas: https://www.geeksforgeeks.org/read-a-parquet-file-using-pandas/\n",
    "\n",
    "- Pyarrow - read_table(): https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_table.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
